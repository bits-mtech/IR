{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IR Assignment Group-082\n",
    "\n",
    "## Group members\n",
    "\n",
    "<style>\n",
    "table {\n",
    "  font-family: arial, sans-serif;\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "  border: 1px solid #dddddd;\n",
    "  text-align: left;\n",
    "  padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even) {\n",
    "  background-color: #dddddd;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Full Name</th>\n",
    "    <th>BITS ID</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>T V Sriharini</td>\n",
    "    <td>2020fc04020@wilp.bits-pilani.ac.in</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Subodh Kant Mishra</td>\n",
    "    <td>2020fc04064@wilp.bits-pilani.ac.in</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mriganka Shekhar Gayen</td>\n",
    "    <td>2020fc04069@wilp.bits-pilani.ac.in</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Given a Dataset collected from the Centre for Inventions and Scientific Information (\"CISI\") and consists of text data about 1,460 documents and 112 associated queries. Its purpose is to be used to build models of information retrieval where a given query will return a list of document IDs relevant to the query. The file \"CISI.REL\" contains the correct list (ie. \"gold standard\" or \"ground proof\") of query-document matching and your model can be compared against this \"gold standard\" to see how it has performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data set and import necessary libraries (1 Marks)\n",
    "We import various packages used for text and general data processing\n",
    "* Pandas, Numpy, Skleaarn - For Data Processing\n",
    "* NLTK - Natural Language Tool Kit for Natural Language Processing\n",
    "* OS, functools - General Packages for Processing files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kants\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kants\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Library imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#we will use Scikit-Learn's TfidfVectorizer, which operates similarly to CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#implementation using scikit-learn\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "import os, glob, re, sys, random, unicodedata, collections,math\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing the files - All FIle\n",
    "\n",
    "We are importing the CISI.ALL file where we perform text processing such as removing newline characters, and '.' characters,\n",
    "and split the data into different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path,filetype=\"ALL\"):\n",
    "    \"\"\"\n",
    "    Extracts text from a given file path.\n",
    "    \"\"\"\n",
    "\n",
    "    qry_id=0\n",
    "    doc_id=0\n",
    "    doc_text = \"\"\n",
    "    rel_set = {}\n",
    "    doc_set = {}\n",
    "    qry_set = {}\n",
    "    lines = \"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        if filetype == \"ALL\" or filetype == \"QRY\":\n",
    "            for l in f.readlines():\n",
    "                lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "            lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "        elif filetype == \"REL\":\n",
    "            for l in f.readlines():\n",
    "                qry_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]) -1\n",
    "                doc_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])-1\n",
    "                if qry_id in rel_set:\n",
    "                    rel_set[qry_id].append(doc_id)\n",
    "                else:\n",
    "                    rel_set[qry_id] = []\n",
    "                    rel_set[qry_id].append(doc_id)\n",
    "            return rel_set\n",
    "        else:\n",
    "            print(\"Invalid file type\")\n",
    "            return lines\n",
    "    if filetype == \"ALL\": \n",
    "        for l in lines:\n",
    "            if l.startswith(\".I\"):\n",
    "                doc_id = int(l.split(\" \")[1].strip())-1\n",
    "            elif l.startswith(\".X\"):\n",
    "                doc_set[doc_id] = doc_text.lstrip(\" \")\n",
    "                doc_id = \"\"\n",
    "                doc_text = \"\"\n",
    "            else:\n",
    "                doc_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.\n",
    "        return doc_set\n",
    "    elif filetype == \"QRY\":\n",
    "        qry_id = \"\"\n",
    "        for l in lines:\n",
    "            if l.startswith(\".I\"):\n",
    "                qry_id = int(l.split(\" \")[1].strip()) -1\n",
    "            elif l.startswith(\".W\"):\n",
    "                qry_set[qry_id] = l.strip()[3:]\n",
    "                qry_id = \"\"\n",
    "        return qry_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1460 documents\n"
     ]
    }
   ],
   "source": [
    "### Processing DOCUMENTS\n",
    "BASE_FOLDER = f\"{os.getcwd()}\\\\archive\\\\\"\n",
    "doc_set = extract_text(f'{BASE_FOLDER}CISI.ALL',filetype=\"ALL\")\n",
    "print(f'Read {len(doc_set)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importing the files - Queries FIle\n",
    "\n",
    "We are importing the CISI.QRY file where we perform text processing such as removing newline characters, and '.' characters,\n",
    "and split the data into different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 112 documents\n"
     ]
    }
   ],
   "source": [
    "qry_set = extract_text(f'{BASE_FOLDER}CISI.QRY',filetype=\"QRY\")\n",
    "print(f'Read {len(qry_set)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Importing the files - Rel FIle\n",
    "\n",
    "We are importing the CISI.QREL file where we perform text processing such as removing newline characters, and '.' characters,\n",
    "and split the data into different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 76 documents\n"
     ]
    }
   ],
   "source": [
    "rel_set = extract_text(f'{BASE_FOLDER}CISI.REL',filetype=\"REL\")\n",
    "print(f'Read {len(rel_set)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract features from text files. Each unique word in the document can be considered as a feature (1 Mark)\n",
    "\n",
    "We check the General statistics of the file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1460 documents, 112 queries and 76 mappings from CISI dataset\n",
      "Average 40.97 and 1 min number of relevant documents by query \n",
      "Queries without relevant documents:  [ 35  37  39  46  47  50  52  58  59  62  63  67  69  71  72  73  74  76\n",
      "  77  79  82  84  85  86  87  88  90  92  93 102 104 105 106 107 109 111]\n"
     ]
    }
   ],
   "source": [
    "## Here we check some statistics and info of CISI dataset\n",
    "\n",
    "print(f'Read {len(doc_set)} documents, {len(qry_set)} queries and {len(rel_set)} mappings from CISI dataset')\n",
    "\n",
    "number_of_rel_docs = [len(value) for key, value in rel_set.items()]\n",
    "print('Average %.2f and %d min number of relevant documents by query ' % \n",
    "      (np.mean(number_of_rel_docs), np.min(number_of_rel_docs)))\n",
    "\n",
    "print('Queries without relevant documents: ', \n",
    "      np.setdiff1d(list(qry_set.keys()),list(rel_set.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the sample of relevant document from query and its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID 14 ==> How much do information retrieval and dissemination systems, as well as automated libraries, cost? Are they worth it to the researcher and to industry?\n",
      "Documents relevants to Query ID 14 [17, 26, 35, 48, 55, 58, 66, 73, 82, 125, 157, 163, 166, 191, 213, 221, 222, 249, 280, 291, 294, 298, 306, 330, 335, 337, 347, 364, 365, 366, 367, 371, 380, 445, 457, 464, 465, 481, 489, 490, 494, 496, 506, 519, 527, 590, 593, 622, 628, 638, 689, 719, 722, 723, 726, 727, 730, 778, 821, 833, 838, 847, 848, 864, 871, 896, 1099, 1160, 1247, 1304, 1352, 1357, 1362, 1365, 1367, 1370, 1371, 1373, 1374, 1375, 1376, 1409]\n",
      "Document ID 48 ==> Adaptive Information Dissemination Sage, C.R. Anderson, R.R. Fitzwater, D.R. Computer dissemination of information offers significant advantages over manual dissemination because the computer can use strategies that are impractical and in some cases impossible for a human.. This paper describes the Ames Laboratory Selective Dissemination of Information system with emphasis on the effectiveness of user feedback.. The system will accept any document, abstract, keyword, etc., in a KWIC or Science Citation Index Source format.. User profiles consist of words or word clusters each with an initially assigned significance value.. These values are used in making the decision to notify a user that he may be interested in a particular document.. According to responses, the significance values are increased or decreased and quickly attain an equilibrium which accurately describes the user's interests.. The system is economical compared to other existing SDI systems and human intervention is negligible except for adding and deleting profile entries.. \n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "idx = random.sample(rel_set.keys(),1)[0]\n",
    "\n",
    "print('Query ID %s ==>' % idx, qry_set[idx])\n",
    "rel_docs = rel_set[idx]\n",
    "print('Documents relevants to Query ID %s' % idx, rel_docs)\n",
    "sample_document_idx = random.sample(rel_docs,1)[0]\n",
    "print('Document ID %s ==>' % sample_document_idx, doc_set[sample_document_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving Feature using Count Vectorizer Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# put the common code into several methods\n",
    "def get_keywords(idx,docs_test,cv,feature_names):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords,docs_body):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Body=====\")\n",
    "    print(docs_body[idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data\n",
    "    \n",
    "def calculate_similarity(X, vectorizor, query, top_k=10):\n",
    "    \"\"\" Vectorizes the `query` via `vectorizor` and calculates the cosine similarity of \n",
    "    the `query` and `X` (all the documents) and returns the `top_k` similar documents.\"\"\"\n",
    "    \n",
    "    # Vectorize the query to the same length as documents\n",
    "    query_vec = vectorizor.transform(query)\n",
    "    # Compute the cosine similarity between query_vec and all the documents\n",
    "    cosine_similarities = cosine_similarity(X,query_vec).flatten()\n",
    "    # Sort the similar documents from the most similar to less similar and return the indices\n",
    "    most_similar_doc_indices = np.argsort(cosine_similarities, axis=0)[:-top_k-1:-1]\n",
    "    return (most_similar_doc_indices, cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>process_text</th>\n",
       "      <th>unique_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18 Editions of the Dewey Decimal Classificatio...</td>\n",
       "      <td>eighteen edit dewey decim classif comaromi pr...</td>\n",
       "      <td>{will, his, attempt, 1971,, have, describe, Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Use Made of Technical Libraries Slater, M. Thi...</td>\n",
       "      <td>use made technic librari slater report analys...</td>\n",
       "      <td>{Taking, technology, is, with, Kingdom., use.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two Kinds of Power An Essay on Bibliographic C...</td>\n",
       "      <td>two kind power essay bibliograph control wils...</td>\n",
       "      <td>{storehouses, will, As, written, familiar, Two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Systems Analysis of a University Library; fina...</td>\n",
       "      <td>system analysi univ librari final report rese...</td>\n",
       "      <td>{Repot, provoked, new, Libraries, Analysis, at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Library Management Game: a report on a resea...</td>\n",
       "      <td>librari manag game report research project br...</td>\n",
       "      <td>{fields, professional, courses, main, times,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>World Dynamics Forrester, J.W. Over the last s...</td>\n",
       "      <td>world dynam forrest last sever decad interest...</td>\n",
       "      <td>{economic, believe, most, world-wide, last, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>World Trends in Library Education Bramley, G. ...</td>\n",
       "      <td>world trend librari educ bramley one signif a...</td>\n",
       "      <td>{new, professional, implications, had, while, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>Legal Restrictions on Exploitation of the Pate...</td>\n",
       "      <td>legal restrict exploit patent monopoli econom...</td>\n",
       "      <td>{remarkable, encouragement, making,, Analysis,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>Language and Thought Poluskin, V.A. This book ...</td>\n",
       "      <td>languag thought poluskin book consid basic as...</td>\n",
       "      <td>{problem, Language, thought,, abstraction,, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>Modern Integral Information Systems for Chemis...</td>\n",
       "      <td>modern integr inform system chemistri chemic ...</td>\n",
       "      <td>{time,, new, compounds, relate, Modern, public...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     18 Editions of the Dewey Decimal Classificatio...   \n",
       "1     Use Made of Technical Libraries Slater, M. Thi...   \n",
       "2     Two Kinds of Power An Essay on Bibliographic C...   \n",
       "3     Systems Analysis of a University Library; fina...   \n",
       "4     A Library Management Game: a report on a resea...   \n",
       "...                                                 ...   \n",
       "1455  World Dynamics Forrester, J.W. Over the last s...   \n",
       "1456  World Trends in Library Education Bramley, G. ...   \n",
       "1457  Legal Restrictions on Exploitation of the Pate...   \n",
       "1458  Language and Thought Poluskin, V.A. This book ...   \n",
       "1459  Modern Integral Information Systems for Chemis...   \n",
       "\n",
       "                                           process_text  \\\n",
       "0      eighteen edit dewey decim classif comaromi pr...   \n",
       "1      use made technic librari slater report analys...   \n",
       "2      two kind power essay bibliograph control wils...   \n",
       "3      system analysi univ librari final report rese...   \n",
       "4      librari manag game report research project br...   \n",
       "...                                                 ...   \n",
       "1455   world dynam forrest last sever decad interest...   \n",
       "1456   world trend librari educ bramley one signif a...   \n",
       "1457   legal restrict exploit patent monopoli econom...   \n",
       "1458   languag thought poluskin book consid basic as...   \n",
       "1459   modern integr inform system chemistri chemic ...   \n",
       "\n",
       "                                            unique_word  \n",
       "0     {will, his, attempt, 1971,, have, describe, Cl...  \n",
       "1     {Taking, technology, is, with, Kingdom., use.,...  \n",
       "2     {storehouses, will, As, written, familiar, Two...  \n",
       "3     {Repot, provoked, new, Libraries, Analysis, at...  \n",
       "4     {fields, professional, courses, main, times,, ...  \n",
       "...                                                 ...  \n",
       "1455  {economic, believe, most, world-wide, last, fo...  \n",
       "1456  {new, professional, implications, had, while, ...  \n",
       "1457  {remarkable, encouragement, making,, Analysis,...  \n",
       "1458  {problem, Language, thought,, abstraction,, re...  \n",
       "1459  {time,, new, compounds, relate, Modern, public...  \n",
       "\n",
       "[1460 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame.from_dict(doc_set, orient='index',columns=['text'])\n",
    "data['process_text'] = data['text'].apply(lambda x:preprocess(x))\n",
    "data['unique_word'] = data['text'].apply(lambda x:set(x.split()))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using scikit Learn Count Vectorizer Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Zeros doc matrix 28708\n",
      "First 10 Vocabulary from document set:  ['eighteen', 'edit', 'dewey', 'decim', 'classif', 'comaromi', 'present', 'studi', 'histori', 'first']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12k</th>\n",
       "      <th>14th</th>\n",
       "      <th>15th</th>\n",
       "      <th>1961b</th>\n",
       "      <th>1967a</th>\n",
       "      <th>1971and</th>\n",
       "      <th>24th</th>\n",
       "      <th>2d</th>\n",
       "      <th>2x2</th>\n",
       "      <th>2x8</th>\n",
       "      <th>...</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zipf</th>\n",
       "      <th>zipfian</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolog</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zund</th>\n",
       "      <th>zvezhinskii</th>\n",
       "      <th>zyabrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 6750 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      12k  14th  15th  1961b  1967a  1971and  24th  2d  2x2  2x8  ...  \\\n",
       "0       0     0     0      0      0        0     0   0    0    0  ...   \n",
       "1       0     0     0      0      0        0     0   0    0    0  ...   \n",
       "2       0     0     0      0      0        0     0   0    0    0  ...   \n",
       "3       0     0     0      0      0        0     0   0    0    0  ...   \n",
       "4       0     0     0      0      0        0     0   0    0    0  ...   \n",
       "...   ...   ...   ...    ...    ...      ...   ...  ..  ...  ...  ...   \n",
       "1455    0     0     0      0      0        0     0   0    0    0  ...   \n",
       "1456    0     0     0      0      0        0     0   0    0    0  ...   \n",
       "1457    0     0     0      0      0        0     0   0    0    0  ...   \n",
       "1458    0     0     0      0      0        0     0   0    0    0  ...   \n",
       "1459    0     0     0      0      0        0     0   0    0    0  ...   \n",
       "\n",
       "      zimmerman  zipf  zipfian  zipper  zone  zoolog  zuckerman  zund  \\\n",
       "0             0     0        0       0     0       0          0     0   \n",
       "1             0     0        0       0     0       0          0     0   \n",
       "2             0     0        0       0     0       0          0     0   \n",
       "3             0     0        0       0     0       0          0     0   \n",
       "4             0     0        0       0     0       0          0     0   \n",
       "...         ...   ...      ...     ...   ...     ...        ...   ...   \n",
       "1455          0     0        0       0     0       0          0     0   \n",
       "1456          0     0        0       0     0       0          0     0   \n",
       "1457          0     0        0       0     0       0          0     0   \n",
       "1458          0     0        0       0     0       0          0     0   \n",
       "1459          0     0        0       0     0       0          0     0   \n",
       "\n",
       "      zvezhinskii  zyabrev  \n",
       "0               0        0  \n",
       "1               0        0  \n",
       "2               0        0  \n",
       "3               0        0  \n",
       "4               0        0  \n",
       "...           ...      ...  \n",
       "1455            0        0  \n",
       "1456            0        0  \n",
       "1457            0        0  \n",
       "1458            0        0  \n",
       "1459            0        0  \n",
       "\n",
       "[1460 rows x 6750 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count term frequency using CountVectorizer from scikit-learn\n",
    "cv = CountVectorizer(stop_words=set(stopwords.words('english'))) \n",
    "X = cv.fit_transform(data['process_text'])\n",
    "print('Non Zeros doc matrix',cv.transform(data['text']).count_nonzero())\n",
    "df = pd.DataFrame(X.todense(), columns=cv.get_feature_names())\n",
    "print('First 10 Vocabulary from document set: ',list(cv.vocabulary_.keys())[:10])\n",
    "df\n",
    "#word_tokenize(str(preprocess(doc_set[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ScikitLear TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Vocabulary from document set:  ['eighteen', 'edit', 'dewey', 'decim', 'classif', 'comaromi', 'present', 'studi', 'histori', 'ddc']\n",
      "tfidf matrix successfully created. \n",
      "Length of features 6646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12k</th>\n",
       "      <th>14th</th>\n",
       "      <th>15th</th>\n",
       "      <th>1961b</th>\n",
       "      <th>1967a</th>\n",
       "      <th>1971and</th>\n",
       "      <th>24th</th>\n",
       "      <th>2d</th>\n",
       "      <th>2x2</th>\n",
       "      <th>2x8</th>\n",
       "      <th>...</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zipf</th>\n",
       "      <th>zipfian</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolog</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zund</th>\n",
       "      <th>zvezhinskii</th>\n",
       "      <th>zyabrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 6646 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      12k  14th  15th  1961b  1967a  1971and  24th   2d  2x2  2x8  ...  \\\n",
       "0     0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "1     0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "2     0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "3     0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "4     0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "...   ...   ...   ...    ...    ...      ...   ...  ...  ...  ...  ...   \n",
       "1455  0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "1456  0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "1457  0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "1458  0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "1459  0.0   0.0   0.0    0.0    0.0      0.0   0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "      zimmerman  zipf  zipfian  zipper  zone  zoolog  zuckerman  zund  \\\n",
       "0           0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "1           0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "2           0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "3           0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "4           0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "...         ...   ...      ...     ...   ...     ...        ...   ...   \n",
       "1455        0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "1456        0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "1457        0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "1458        0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "1459        0.0   0.0      0.0     0.0   0.0     0.0        0.0   0.0   \n",
       "\n",
       "      zvezhinskii  zyabrev  \n",
       "0             0.0      0.0  \n",
       "1             0.0      0.0  \n",
       "2             0.0      0.0  \n",
       "3             0.0      0.0  \n",
       "4             0.0      0.0  \n",
       "...           ...      ...  \n",
       "1455          0.0      0.0  \n",
       "1456          0.0      0.0  \n",
       "1457          0.0      0.0  \n",
       "1458          0.0      0.0  \n",
       "1459          0.0      0.0  \n",
       "\n",
       "[1460 rows x 6646 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizor = TfidfVectorizer(decode_error='replace', strip_accents='unicode', analyzer='word', \n",
    "                                       stop_words='english', ngram_range=(1, 1), \n",
    "                                       norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True,)\n",
    "X_tf = tfidf_vectorizor.fit_transform(data['process_text'])\n",
    "features = tfidf_vectorizor.get_feature_names()\n",
    "df_tf = pd.DataFrame(X_tf.todense(), columns=features)\n",
    "print('First 10 Vocabulary from document set: ',list(tfidf_vectorizor.vocabulary_.keys())[:10])\n",
    "print('tfidf matrix successfully created.','\\nLength of features',len(features))\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement pseudo-relevance feedback.(1Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ==>  The need to provide personnel for the information field. \n",
      "Relevant documents IDs: ==>  [5, 13, 21, 84, 170, 184, 185, 302, 338, 391, 399, 452, 838, 890, 895, 922, 923, 948, 984, 1019, 1020, 1146, 1148, 1205, 1355] \n",
      " 25  relevant documents\n",
      "bm25 scores ----->  [10.96997353  7.75949152  8.81442548 ...  9.52562468  5.60350587\n",
      "  8.18047688] \n",
      "Length of scores 1460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Hidden Dimension Hall, E.T. Generally speaking, there are two types of books of interest to the serious reader today:  those that are content oriented, designed to convey a particular body of knowledge, and those that deal with structure, the way in which events are organized.  It is doubtful if an author has any control over which of these two types of books he writes, though it is desirable that he be aware of the difference.  The same applies to the reader whose satisfaction depends largely on his unstated expectations.  In today's world, when all of us are overwhelmed with data from many sources, it is easy to understand why people are apt to feel that they are losing touch with developments even in their own field.  One senses that there is also a growing awareness of a loss of relatedness to the world at large.  This loss of relatedness leads to an increased need for organizing frames of reference to aid in intergrating the mass of rapidly changing information with which man must cope. The Hidden Dimension attempts to provide just this. \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 20\n",
    "query = qry_set[idx] #get query text\n",
    "rel_docs = rel_set[idx] #get relevant documents\n",
    "\n",
    "# Index all documents using BM25\n",
    "corpus = list(doc_set.values())\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Process query and get scores for each indexed document using BM25\n",
    "tokenized_query = query.split(\" \")\n",
    "print('Query ==> ', query, '\\nRelevant documents IDs: ==> ', rel_docs, '\\n',len(rel_docs),' relevant documents')\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "print('bm25 scores -----> ',scores,'\\nLength of scores',len(scores))\n",
    "doc_set[scores.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e934a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611111111111111"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Source: https://gist.github.com/bwhite/3726239\n",
    "def mean_reciprocal_rank(bool_results, k=10):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    bool_results = (np.atleast_1d(r[:k]).nonzero()[0] for r in bool_results)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in bool_results])\n",
    "\n",
    "mean_reciprocal_rank([[0, 0, 1], [0, 1, 0], [1, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6311e308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 411  895  584  340  922  113  691  546  947  474 1069 1259 1223  552\n",
      "  132  949  827  247 1409 1204  720 1093  897  141  593  338 1098  597\n",
      "  647   26 1357  124  975  230  670  643  155  184  197  359  284  387\n",
      "  274  492  582  162 1188 1324  382  187 1406 1398  383  403 1439  233\n",
      "  407  240  266  842  366  718  444   16  258  409  127 1429 1361 1156\n",
      "  846  589  123  239    8  464  946 1244  259  882  251 1445  957  631\n",
      "  977  581  938  659 1317  306  511 1356 1413  215    7  480 1351  629\n",
      "  458 1359]\n",
      "[0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "## Argsort gives the indexes of values in increasing order, so we input with the negative values of scores\n",
    "most_relevant_documents = np.argsort(-scores)\n",
    "\n",
    "print(most_relevant_documents[:100]) # printing first 100 most relevant results\n",
    "\n",
    "## Mask relevant documents with 0's and 1's according to query <-> document annotation\n",
    "masked_relevance_results = np.zeros(most_relevant_documents.shape)\n",
    "masked_relevance_results[rel_docs] = 1\n",
    "sorted_masked_relevance_results = np.take(masked_relevance_results, most_relevant_documents)\n",
    "\n",
    "print(sorted_masked_relevance_results[:100]) #printing first 100 results: 1 is relevant 0 isn't\n",
    "\n",
    "# Calculate MRR@10\n",
    "print(mean_reciprocal_rank([sorted_masked_relevance_results]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute TF and TF-IDF factors (1Mark)\n",
    "We use the tfidf transformer to compute the tf and IDF Factors for the dataset\n",
    "##### TF-IDF Calculation using TFIDF Transformer and tfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X_tf)\n",
    "#data = pd.DataFrame.from_dict(doc_set, orient='index',columns=['text'])\n",
    "feature_names=tfidf_vectorizor.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>process_text</th>\n",
       "      <th>unique_word</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18 Editions of the Dewey Decimal Classificatio...</td>\n",
       "      <td>eighteen edit dewey decim classif comaromi pr...</td>\n",
       "      <td>{will, his, attempt, 1971,, have, describe, Cl...</td>\n",
       "      <td>{'ddc': 0.467, 'dewey': 0.438, 'eighteenth': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Use Made of Technical Libraries Slater, M. Thi...</td>\n",
       "      <td>use made technic librari slater report analys...</td>\n",
       "      <td>{Taking, technology, is, with, Kingdom., use.,...</td>\n",
       "      <td>{'slater': 0.355, 'desk': 0.355, 'transfer': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two Kinds of Power An Essay on Bibliographic C...</td>\n",
       "      <td>two kind power essay bibliograph control wils...</td>\n",
       "      <td>{storehouses, will, As, written, familiar, Two...</td>\n",
       "      <td>{'power': 0.469, 'slogan': 0.417, 'wilson': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Systems Analysis of a University Library; fina...</td>\n",
       "      <td>system analysi univ librari final report rese...</td>\n",
       "      <td>{Repot, provoked, new, Libraries, Analysis, at...</td>\n",
       "      <td>{'ugc': 0.533, 'repot': 0.533, 'buckland': 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Library Management Game: a report on a resea...</td>\n",
       "      <td>librari manag game report research project br...</td>\n",
       "      <td>{fields, professional, courses, main, times,, ...</td>\n",
       "      <td>{'game': 0.669, 'women': 0.301, 'spread': 0.24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>World Dynamics Forrester, J.W. Over the last s...</td>\n",
       "      <td>world dynam forrest last sever decad interest...</td>\n",
       "      <td>{economic, believe, most, world-wide, last, fo...</td>\n",
       "      <td>{'world': 0.425, 'interplay': 0.403, 'mutual':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>World Trends in Library Education Bramley, G. ...</td>\n",
       "      <td>world trend librari educ bramley one signif a...</td>\n",
       "      <td>{new, professional, implications, had, while, ...</td>\n",
       "      <td>{'bramley': 0.455, 'potent': 0.408, 'twentieth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>Legal Restrictions on Exploitation of the Pate...</td>\n",
       "      <td>legal restrict exploit patent monopoli econom...</td>\n",
       "      <td>{remarkable, encouragement, making,, Analysis,...</td>\n",
       "      <td>{'legal': 0.409, 'morn': 0.389, 'baxter': 0.38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>Language and Thought Poluskin, V.A. This book ...</td>\n",
       "      <td>languag thought poluskin book consid basic as...</td>\n",
       "      <td>{problem, Language, thought,, abstraction,, re...</td>\n",
       "      <td>{'poluskin': 0.697, 'thought': 0.614, 'complex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>Modern Integral Information Systems for Chemis...</td>\n",
       "      <td>modern integr inform system chemistri chemic ...</td>\n",
       "      <td>{time,, new, compounds, relate, Modern, public...</td>\n",
       "      <td>{'chernyi': 0.69, 'world': 0.464, 'modern': 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     18 Editions of the Dewey Decimal Classificatio...   \n",
       "1     Use Made of Technical Libraries Slater, M. Thi...   \n",
       "2     Two Kinds of Power An Essay on Bibliographic C...   \n",
       "3     Systems Analysis of a University Library; fina...   \n",
       "4     A Library Management Game: a report on a resea...   \n",
       "...                                                 ...   \n",
       "1455  World Dynamics Forrester, J.W. Over the last s...   \n",
       "1456  World Trends in Library Education Bramley, G. ...   \n",
       "1457  Legal Restrictions on Exploitation of the Pate...   \n",
       "1458  Language and Thought Poluskin, V.A. This book ...   \n",
       "1459  Modern Integral Information Systems for Chemis...   \n",
       "\n",
       "                                           process_text  \\\n",
       "0      eighteen edit dewey decim classif comaromi pr...   \n",
       "1      use made technic librari slater report analys...   \n",
       "2      two kind power essay bibliograph control wils...   \n",
       "3      system analysi univ librari final report rese...   \n",
       "4      librari manag game report research project br...   \n",
       "...                                                 ...   \n",
       "1455   world dynam forrest last sever decad interest...   \n",
       "1456   world trend librari educ bramley one signif a...   \n",
       "1457   legal restrict exploit patent monopoli econom...   \n",
       "1458   languag thought poluskin book consid basic as...   \n",
       "1459   modern integr inform system chemistri chemic ...   \n",
       "\n",
       "                                            unique_word  \\\n",
       "0     {will, his, attempt, 1971,, have, describe, Cl...   \n",
       "1     {Taking, technology, is, with, Kingdom., use.,...   \n",
       "2     {storehouses, will, As, written, familiar, Two...   \n",
       "3     {Repot, provoked, new, Libraries, Analysis, at...   \n",
       "4     {fields, professional, courses, main, times,, ...   \n",
       "...                                                 ...   \n",
       "1455  {economic, believe, most, world-wide, last, fo...   \n",
       "1456  {new, professional, implications, had, while, ...   \n",
       "1457  {remarkable, encouragement, making,, Analysis,...   \n",
       "1458  {problem, Language, thought,, abstraction,, re...   \n",
       "1459  {time,, new, compounds, relate, Modern, public...   \n",
       "\n",
       "                                               keywords  \n",
       "0     {'ddc': 0.467, 'dewey': 0.438, 'eighteenth': 0...  \n",
       "1     {'slater': 0.355, 'desk': 0.355, 'transfer': 0...  \n",
       "2     {'power': 0.469, 'slogan': 0.417, 'wilson': 0....  \n",
       "3     {'ugc': 0.533, 'repot': 0.533, 'buckland': 0.4...  \n",
       "4     {'game': 0.669, 'women': 0.301, 'spread': 0.24...  \n",
       "...                                                 ...  \n",
       "1455  {'world': 0.425, 'interplay': 0.403, 'mutual':...  \n",
       "1456  {'bramley': 0.455, 'potent': 0.408, 'twentieth...  \n",
       "1457  {'legal': 0.409, 'morn': 0.389, 'baxter': 0.38...  \n",
       "1458  {'poluskin': 0.697, 'thought': 0.614, 'complex...  \n",
       "1459  {'chernyi': 0.69, 'world': 0.464, 'modern': 0....  \n",
       "\n",
       "[1460 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate tf-idf for all documents in your list. docs_test has 1460 documents\n",
    "tf_idf_vector=tfidf_transformer.transform(tfidf_vectorizor.transform(doc_set.values()))\n",
    "\n",
    "results=[]\n",
    "for i in range(tf_idf_vector.shape[0]):\n",
    "    \n",
    "    # get vector for a single document\n",
    "    curr_vector=tf_idf_vector[i]\n",
    "    \n",
    "    #sort the tf-idf vector by descending order of scores\n",
    "    sorted_items=sort_coo(curr_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,20)\n",
    "    \n",
    "    \n",
    "    results.append(keywords)\n",
    "\n",
    "data['keywords']=results\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrieve the top k- documents using TF-IDF model(2Marks)\n",
    "\n",
    "###### Calculating Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         ... 0.         0.         0.00453827]\n",
      " [0.         1.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         1.         ... 0.07362287 0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.07362287 ... 1.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         1.         0.        ]\n",
      " [0.00453827 0.         0.         ... 0.         0.         1.        ]] \n",
      "cosine similarity for first document\n",
      " [1.         0.         0.         ... 0.         0.         0.00453827]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "print(cosine_similarity(tf_idf_vector),'\\ncosine similarity for first document\\n',cosine_similarity(tf_idf_vector)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Top K(10) Document based on Cosine Similarity for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?']\n",
      "Similar Doc Id [1029 1018   70  446  905  872  804   34  223  743] Cosine Similarity [0.1809991965128494, 0.16793493893911618, 0.14740139478799813, 0.14663589124571264, 0.1458362426448593, 0.1449649543237236, 0.14189811351215142, 0.1398631236465585, 0.1398317961298713, 0.13771136591536717]\n",
      "['How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?']\n",
      "Similar Doc Id [1375 1095  992 1135  126  566  821  373 1376  124] Cosine Similarity [0.287490424302825, 0.23257645111886516, 0.21762007735944966, 0.21732278845991515, 0.21081522188256083, 0.20386203673160094, 0.19490563054561325, 0.1945901474525878, 0.19369750450088435, 0.18912026779332805]\n",
      "['What is information science?  Give definitions where possible.']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['Image recognition and any other methods of automatically transforming printed text into computer-ready form.']\n",
      "Similar Doc Id [1279 1293  526 1408 1223  564 1184  555  331  319] Cosine Similarity [0.3328791224292452, 0.2973445129794269, 0.2821701877795847, 0.2431865412796466, 0.2272945873917323, 0.2202702554680129, 0.2178546876704862, 0.20582953879803836, 0.19952378529153508, 0.19165788203159903]\n",
      "['What special training will ordinary researchers and businessmen need for proper information management and unobstructed use of information retrieval systems? What problems are they likely to encounter?']\n",
      "Similar Doc Id [ 861  482  840 1429 1374  732 1353 1370 1241  939] Cosine Similarity [0.2668666882406612, 0.25365887635969564, 0.19767838106882035, 0.18099991309641722, 0.16254012880954877, 0.15367962993485007, 0.1520418908207444, 0.14829916375752963, 0.1436207457550598, 0.13462769913054168]\n",
      "['What possibilities are there for verbal communication between computers and humans, that is, communication via the spoken word?']\n",
      "Similar Doc Id [ 541  432 1262   70  576  648   50   25  665  477] Cosine Similarity [0.1945746867665849, 0.16304867778153717, 0.1603291383427682, 0.13096316454326445, 0.1276430281993843, 0.12350892219694037, 0.12009554045436391, 0.11774282371012668, 0.11588294279857225, 0.1147546315971159]\n",
      "['Describe presently working and planned systems for publishing and printing original papers by computer, and then saving the byproduct, articles coded in data-processing form, for further use in retrieval.']\n",
      "Similar Doc Id [1427  494 1135  992 1351 1251  375  856  921  520] Cosine Similarity [0.2530305579268741, 0.10636944451529098, 0.10059582443130077, 0.09647026919122934, 0.09571832862655136, 0.09485368677270009, 0.0930158427976022, 0.09206443301313579, 0.08997478957445051, 0.0898666275068278]\n",
      "['Describe information retrieval and indexing in other languages. What bearing does it have on the science in general?']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['What possibilities are there for automatic grammatical and contextual analysis of articles for inclusion in an information retrieval system?']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['The use of abstract mathematics in information retrieval, e.g. group theory.']\n",
      "Similar Doc Id [1280   34  314  587  671  389 1299  728    5 1403] Cosine Similarity [0.28941129052803505, 0.21236992937442814, 0.20853008542050191, 0.19445196258205516, 0.19411668402103818, 0.18274112516656865, 0.18250749273969047, 0.17619675033300963, 0.17602445100167852, 0.17513893721529045]\n",
      "['What is the need for information consolidation, evaluation, and retrieval in scientific research?']\n",
      "Similar Doc Id [ 964  770 1102 1101  455  936 1109  151   28 1360] Cosine Similarity [0.28820075422075375, 0.23090944257171564, 0.21938390553731438, 0.20526589908591644, 0.20441358702146534, 0.19920444985568814, 0.19855828059347913, 0.19627090310058087, 0.18937006078599888, 0.1843444339967637]\n",
      "['Give methods for high speed publication, printing, and distribution of scientific journals.']\n",
      "Similar Doc Id [ 966  738   88  593   22  898  103  715 1097  883] Cosine Similarity [0.18983642265851763, 0.18425506593207666, 0.1761983528342014, 0.1388585523801491, 0.13025464597542885, 0.12086013322421242, 0.12081506265004281, 0.11464854864909768, 0.11134152012295379, 0.11027331916366577]\n",
      "['What criteria have been developed for the objective evaluation of information retrieval and dissemination systems?']\n",
      "Similar Doc Id [1100 1375   49  982  193  657  152 1374 1133  564] Cosine Similarity [0.3750318027245247, 0.2999749796523676, 0.2684980034400632, 0.25405788470343915, 0.24748846778501077, 0.24288856210643547, 0.223485327997829, 0.2184941845479586, 0.21717141833406112, 0.19727612831390043]\n",
      "['What future is there for automatic medical diagnosis?']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['How much do information retrieval and dissemination systems, as well as automated libraries, cost? Are they worth it to the researcher and to industry?']\n",
      "Similar Doc Id [ 254  514 1253  614  154  590  291   82  973  859] Cosine Similarity [0.28310848425590024, 0.2593290001321443, 0.20092543127754917, 0.1615102436799115, 0.15695578545551772, 0.14769299296302543, 0.13713068628513295, 0.13613639927928034, 0.13521227589499865, 0.12670890023021514]\n",
      "['What systems incorporate multiprogramming or remote stations in information retrieval?  What will be the extent of their use in the future?']\n",
      "Similar Doc Id [1306  109  115  203  459 1017   22  258  311  491] Cosine Similarity [0.4261170817389289, 0.17360270592710653, 0.17304014529989245, 0.16097499637024446, 0.15732576374310245, 0.1534621261519227, 0.1505810440162763, 0.13844356538967467, 0.13009918311810334, 0.12256757880783403]\n",
      "['Means of obtaining large volume, high speed, customer usable information retrieval output.']\n",
      "Similar Doc Id [ 966  738  103   88  879  511   76  101  593 1370] Cosine Similarity [0.1606552807818327, 0.15593187512830906, 0.15224881682339902, 0.1491136181953593, 0.13394620657672898, 0.12637866135315484, 0.12414250568098693, 0.1211915663131084, 0.11751359095994184, 0.1116720712427178]\n",
      "['What methods are there for encoding, automatically matching, and automatically drawing structures extended in two dimensions, like the structural formulas for chemical compounds?']\n",
      "Similar Doc Id [1256 1032  627  606 1083 1446  974  450 1171  143] Cosine Similarity [0.26437865931106014, 0.21399588802197436, 0.16316770765923036, 0.1580840886074293, 0.1550707151803803, 0.1550707151803803, 0.14870142804452582, 0.14697261420462812, 0.14176937820878746, 0.1396406634855399]\n",
      "['Techniques of machine matching and machine searching systems. Coding and matching methods.']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['Testing automated information systems.']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['The need to provide personnel for the information field.']\n",
      "Similar Doc Id [1204 1014  338 1320  895  474 1205  215  922  136] Cosine Similarity [0.3164192739110692, 0.23580639832696104, 0.2006342666949733, 0.195556901647103, 0.19073543221168549, 0.18781137939832293, 0.14995620741760807, 0.14729848405856114, 0.14378363419888207, 0.14121978486094622]\n",
      "['Automated information in the medical field.']\n",
      "Similar Doc Id [1411  132 1127 1113  115 1342  769   36  552  106] Cosine Similarity [0.2737023239551293, 0.2168821334360094, 0.20053392759358188, 0.2003747251272618, 0.19287085652780164, 0.18778722935675832, 0.1875435323384737, 0.18053831164260423, 0.17766401254404168, 0.1736465528402883]\n",
      "['Amount of use of books in libraries. Relation to need for automated information systems .']\n",
      "Similar Doc Id [  28 1102 1360 1101  136  151  964  770 1445  912] Cosine Similarity [0.24742837690701122, 0.24343307904353295, 0.2408619604722292, 0.22776743678959055, 0.21918875778764763, 0.21778639664295124, 0.1991948615221271, 0.18274893543239965, 0.17860085466472087, 0.17823084858715768]\n",
      "['Educational and training requirements for personnel in the information field. Possibilities for this training.  Needs for programs providing this training.']\n",
      "Similar Doc Id [1204 1014  474  338 1320  895 1411  284  215   26] Cosine Similarity [0.34891158694183655, 0.2600207744437928, 0.20709726566789835, 0.19621451023019582, 0.18398229551175874, 0.15452486193714893, 0.1392461568660046, 0.13570832971680608, 0.13358134587195775, 0.13063712451673556]\n",
      "['International systems for exchange and dissemination of information.']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['Cost and determination of cost associated with systems of automated information.']\n",
      "Similar Doc Id [ 614  590  291   82  973  859 1373  821  833  638] Cosine Similarity [0.34656745572061176, 0.31691850394579246, 0.294253986398827, 0.29212045288402827, 0.2901374759361564, 0.27189099693869373, 0.26667622121267875, 0.24829407112595753, 0.2474581172944682, 0.24381908639343455]\n",
      "['Computerized information retrieval systems.  Computerized indexing systems.']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['Computerized information systems in fields related to chemistry.']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['Specific advantages of computerized index systems.']\n",
      "Similar Doc Id [1282 1009  376  564   50  193   52  388  145  789] Cosine Similarity [0.28810334957119926, 0.2768334079176039, 0.27425827623488064, 0.25742392531741637, 0.25596852520459823, 0.2537350211607494, 0.2527985542029859, 0.24301104242732188, 0.24141827686292536, 0.238706766098698]\n",
      "['Information dissemination by journals and periodicals.']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['Information systems in the physical sciences.']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['Attempts at computerized and mechanized systems for general libraries. Problems and methods of automated general author and title indexing systems.']\n",
      "Similar Doc Id [ 875   51  138  245  279  275  782   87 1404  798] Cosine Similarity [0.23282999222809317, 0.2060388381467881, 0.19962313512005067, 0.19649446029043469, 0.18088077537958736, 0.17673681902522403, 0.17628988587987354, 0.1588808759485841, 0.1523185590038347, 0.14917897990593895]\n",
      "['Retrieval systems which provide for the automated transmission of information to the user from a distance.']\n",
      "Similar Doc Id [1360  869 1403  455 1106  614  369  817   45  501] Cosine Similarity [0.2732308490706533, 0.25041163189798354, 0.22806520725739213, 0.22278366501600308, 0.21857788797511454, 0.19641308524477175, 0.1962597939590185, 0.1922659553026797, 0.1860109938334171, 0.1819884549285991]\n",
      "['Methods of coding used in computerized index systems.']\n",
      "Similar Doc Id [1282 1009  376  564   50  193   52  388  145  789] Cosine Similarity [0.28810334957119926, 0.2768334079176039, 0.27425827623488064, 0.25742392531741637, 0.25596852520459823, 0.2537350211607494, 0.2527985542029859, 0.24301104242732188, 0.24141827686292536, 0.238706766098698]\n",
      "['Government supported agencies and projects dealing with information dissemination.']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['What are some of the theories and practices in computer translating of texts from one national language to another?  How can machine translating compete with traditional methods of translating in comprehending nuances of meaning in languages of different structures?']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['What lists of words useful for indexing or classifying material are available?  Wanted are lists of terms that are descriptive vocabularies of particular fields or schedules of words that are related to each other in meaningful schemes.  Wanted are lists that have been tested, at least to some extent, and found useful for organizing material and for retrieving it.']\n",
      "Similar Doc Id [1306  552  109  115  398  734  459  228  640   22] Cosine Similarity [0.36722099752581366, 0.15225908205193703, 0.1496080809142284, 0.14912327501569164, 0.1264648779241404, 0.12146661156935587, 0.11946205223833395, 0.11839171680555431, 0.11671138396800393, 0.11434058935031824]\n",
      "['How can access words in an information retrieval system be kept up to date? Word meanings and usage often change and lists must be dynamic to be current. What definitions of the problem and progress toward solutions have been made in providing necessary flexibility in systems of subject headings, index words, or other symbols used for getting at stored data?']\n",
      "Similar Doc Id [  33  314   85 1214   50 1143  211  576  202 1239] Cosine Similarity [0.21801052343329105, 0.17059841615646698, 0.16583086101567845, 0.1607347754299256, 0.15829082292708652, 0.15017489333644007, 0.14839157086319088, 0.14303134507916546, 0.14108047081735134, 0.13786150723646523]\n",
      "['The progress of information retrieval presents problems of maladjustment and dislocation of personnel.  Training and retraining of people to use the new equipment is important at all levels.  Librarians, assistants, technicians, students, researchers, and even executives will need education to learn the purpose, values, and uses of information systems and hardware. What programs have been developed to change the attitudes and skills of traditional workers and help them to learn the newer techniques?']\n",
      "Similar Doc Id [1204 1133  343  321  715  846 1046  230  906  122] Cosine Similarity [0.26505194021159284, 0.22576502447509814, 0.20234110807437655, 0.1615460066763629, 0.160582139944977, 0.1547727775188396, 0.1431698690331424, 0.1415695267192102, 0.14032915087578907, 0.13755121533811016]\n",
      "['What is the status of machine translation?  What progress has been made in the use of computers to transfer from one language to another with some degree of automation?  What problems and stumbling blocks have been found and are they considered to be insurmountable limitations or only challenging to the field of documentation on an international scale?']\n",
      "Similar Doc Id [   1  721 1011 1294  137  481  768  898  359  534] Cosine Similarity [0.20610107352031265, 0.2031487158676731, 0.18877750265300083, 0.1812797779203351, 0.17529212705074387, 0.17157706203341316, 0.1633513523816898, 0.163170732732191, 0.15394764523946847, 0.1445372003415628]\n",
      "['Is alphabetical ordering of material considered to be a useful tool in information retrieval?  What studies have been done to compare the effectiveness of alphabetical order with other organization schemes? Is there a generally accepted form of arranging material in alphabetical order, and is there an easy way of achieving this form without going to a great amount of effort?']\n",
      "Similar Doc Id [  61 1013    2  463  131 1182   57 1104  262  769] Cosine Similarity [0.16869569111523644, 0.14952579030032012, 0.14708294811196598, 0.13848459034261512, 0.13068301245161973, 0.12805261814631946, 0.12766654141121106, 0.12753731476729582, 0.12675321781723833, 0.1244141152998959]\n",
      "['The average student or researcher has difficulty in comprehending the vocabulary of information retrieval.  It appears important that this new field be understood before it is to be fully accepted.  What basic articles would provide an understanding of the various important aspects of the information storage and retrieval?']\n",
      "Similar Doc Id [1411  130  100  293  404  412  461 1316  106  132] Cosine Similarity [0.23347554519202568, 0.17547184047034428, 0.15066243849336391, 0.14979403261289925, 0.14722233164626655, 0.14544255180300042, 0.1432390060416377, 0.13432883099800325, 0.12971483888833024, 0.1283457188141104]\n",
      "['The difficulties encountered in information retrieval systems are often less related to the equipment used than to the failure to plan adequately for document analysis, indexing, and machine coding.  The position of the programmer is to take a problem and write it in a way in which the equipment will understand.  What articles have been written describing research in maximizing the effectiveness of programming?']\n",
      "Similar Doc Id [   2  818  711 1443 1049  655 1402  313 1427 1261] Cosine Similarity [0.21196899373256894, 0.18367543400623365, 0.16905302249221923, 0.16323608742256887, 0.1617303647495082, 0.15760368391722604, 0.14655778519081425, 0.14343573131488965, 0.14052485963610098, 0.13560135289852043]\n",
      "['There are presently fifty to one hundred technical journals being published.  On the average, two new journals appear every day.  In the many journals published, one to two million articles appear every year.  What attempts have been made to cope with this amount of scientific and technical publication in terms of analysis, control, storage, and retrieval?']\n",
      "Similar Doc Id [1431   63  874 1008  148  370  618 1272  920  820] Cosine Similarity [0.13625943289407771, 0.13590508966171283, 0.13102535200823143, 0.1303837138486428, 0.12351483600751292, 0.12266981783820852, 0.1187445308067009, 0.11809125044330836, 0.11780672702994771, 0.11112620787311105]\n",
      "['I am looking for information about the impact of automation on libraries and its significance for libraries in general.  This includes the increasing importance of automation in view of the proliferation of information today, and how automation can help libraries cope with this problem.  How will automation affect libraries and how should they react to the idea of automation?']\n",
      "Similar Doc Id [ 301  718  411 1220  610  111  880   23  820 1290] Cosine Similarity [0.15952950718150272, 0.1340252614290397, 0.12321780926190157, 0.1217156929784558, 0.12023432452429883, 0.1167109087458793, 0.11465151827034104, 0.11231095736074459, 0.11218674468307693, 0.10558274128909587]\n",
      "['I am seeking information on the use of data processing in libraries and the mechanization of routine library processes and procedures.  I would like descriptions of both general and specific applications of automation in such areas as circulation, cataloging, acquisitions, serial records, and other record-keeping.  Examples should be based on the operation of a conventional public or university library, or practices in a special library which could also be applied in a public or university library.  Give descriptions of equipment and operations, both present and projected.']\n",
      "Similar Doc Id [ 903 1323  921  876  998  540   63  542  887  864] Cosine Similarity [0.3077935148542685, 0.2944508761953029, 0.2942930275052336, 0.28126209981000233, 0.2396644520082782, 0.23174804632160412, 0.2313931805590216, 0.21868424258290595, 0.20856502439130367, 0.20318652837246845]\n",
      "['Is there any established means at present for an international exchange of material about information retrieval?  If there is, does it take the form of an international agency or center which regularly distributes information retrieval methods and research results?  If there is not, in what ways has this material crossed national boundaries?  What seem to have been some of the problems blocking a better international exchange, and is any effort being made to solve some of those problems?']\n",
      "Similar Doc Id [ 437  888  249   73 1281  534  691  731 1371  393] Cosine Similarity [0.22730240749214048, 0.20520939945825478, 0.18077803692024919, 0.16753371745484064, 0.15979520734964867, 0.15694477774835477, 0.1530279834373267, 0.15190235182416206, 0.14600825740985995, 0.14580513222460864]\n",
      "['Information retrieval is still such a new and experimental field that a line distinguishing research and practice is often difficult - even impossible - to draw.  Are there, however, actual centers of research on information retrieval?  If so, in which countries are they located?  Who supports them - government, business, universities, or libraries?  Can information retrieval as a specialized research discipline be said to be emerging, or is it still an amalgam of skills from other fields, such as mathematics, engineering, and library science?  In other words, tell me about information retrieval research.']\n",
      "Similar Doc Id [ 115 1150 1182 1256  162  509  130  641 1295 1453] Cosine Similarity [0.17494584609458497, 0.13858062783081251, 0.12817328074243972, 0.12791439612302263, 0.12657608216794108, 0.12370212537502777, 0.11813477594434521, 0.11614840883193983, 0.10934065719269581, 0.10574871674427848]\n",
      "['Most resources have been spent on applying information retrieval techniques to the physical and medical sciences.  But, has information retrieval been used at all in the natural sciences, social sciences, and humanities?  If so, what have been some of the problems which have been encountered with these subject areas and how have they been solved, if at all?  Have the characteristics of these subject areas necessitated the development of new information retrieval techniques? What are the prospcts for future machine control in these areas?']\n",
      "Similar Doc Id [1095  788 1101  951  474 1344 1169  714 1009  787] Cosine Similarity [0.24780478383934376, 0.16955278031273796, 0.16656848876080044, 0.16285028178550737, 0.1528186342792713, 0.14547172594402413, 0.13543498006102145, 0.13511778602729274, 0.13172851818364725, 0.1263556119936764]\n",
      "['Is there any use for traditional classification schemes - DDC, UDC, LC, etc. - in information retrieval systems?  If there is, which scheme appears most suited to machine use and where has it been applied? If there is not, why are these classification schemes irrelevant? Has research shown that a subject classification of knowledge is completely unnecessary in machine systems? Or, have new schemes been devised which appear to be more suited to machine use?']\n",
      "Similar Doc Id [1429 1441 1074  988  526  800    0  837  289 1229] Cosine Similarity [0.29989961494996226, 0.21030895158777912, 0.20134646328967548, 0.1804632692499561, 0.17826970955788574, 0.17195707241965114, 0.16805947168403076, 0.14031085154459574, 0.1341864570571724, 0.1275243970168883]\n",
      "['Coordinate indexing utilizes descriptors for controlled language.  Of what use are descriptors in the construction of an index?  How can descriptors be used for searching in an information retrieval system?']\n",
      "Similar Doc Id [ 145  376   52   50  193  388 1282  509 1009  488] Cosine Similarity [0.2884157862159374, 0.2815083281986762, 0.2545428592240318, 0.2526191536459512, 0.24489209914545398, 0.2448236063625028, 0.24115675193951136, 0.2362529142314142, 0.23172325341277103, 0.23161418720425075]\n",
      "['What are the characteristics of MEDLARS (Medical Literature Analysis and Retrieval System) project which has been undertaken by the National Library of Medicine?  How does it index current medical journals and of what relation is this indexing system to Index Medicus? What are the major components of the MEDLARS project and its major operating details?']\n",
      "Similar Doc Id [ 467 1254  133   40 1038  555  888  194  193  881] Cosine Similarity [0.3598578907723509, 0.22502256594471962, 0.1893296959181085, 0.18534475784703425, 0.18341784387475546, 0.18186387511012295, 0.17948816615457372, 0.1711448062339877, 0.17101013356583328, 0.16672411741960402]\n",
      "['How can the computer be used in medical science for diagnostic and clinical record keeping purposes?  Have any programs of automation been tried in hospitals?  If so, what have been the results? What problems have been encountered in the use of automation in medicine?  For what purposes can an automated system of clinical records be used?  What are other possible uses of the computer in medicine?']\n",
      "Similar Doc Id [ 906  317   35  886  146  875  870  851  192 1078] Cosine Similarity [0.29640468386580837, 0.26587917865499444, 0.23141021318328583, 0.22803257242172656, 0.22193309794119548, 0.21956641659193668, 0.21247795446817183, 0.20273251062459793, 0.19452303929535053, 0.19390868282244125]\n",
      "['What is the effect on librarians of automation?  Note the new types of technology to be used in the library which will have an effect on the status, position, and function of the librarians.  What changes are being contemplated or have been initiated to introduce automation into the education of librarians?']\n",
      "Similar Doc Id [ 292  590  685  638  644 1109  785 1281  958  148] Cosine Similarity [0.17831893124846412, 0.16652638340790352, 0.15920564199285142, 0.15119176913980423, 0.14974297756715388, 0.14319907689706532, 0.1388860673350404, 0.1357559953790808, 0.13157254723851902, 0.1305715643359859]\n",
      "['What are the aims and objectives of the medical literature analysis and retrieval system (MEDLARS)?  How does MEDLARS operate?  What are the possible applications of MEDLARS to future information retrieval systems?']\n",
      "Similar Doc Id [1459  478  480  481  482  483  484  485  486  487] Cosine Similarity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[\"The standard method of finding information in today's libraries is through the use of the alphabetically arranged card catalog or the classified catalog based on a classification system such as the DC or LC.  Can these systems be modified for use with automated information retrieval?\"]\n",
      "Similar Doc Id [ 940  208  274  997  275  939  871  885  298 1251] Cosine Similarity [0.26046362702603926, 0.2500435562636844, 0.23745083478249934, 0.23438489217417874, 0.23034570709291716, 0.2275386855577004, 0.20770000978120698, 0.20563539402716155, 0.20189124422899962, 0.20171539838323163]\n",
      "['In catalogs which are either arranged alphabetically or arranged by classification number, the LC entry, printed in readable language, is ultimately important because the individual looking for information has a definite author, title, or subject phrase in his language (probably English in our case) in mind.  Will LC entries and subject headings be used in the same manner in automated systems?']\n",
      "Similar Doc Id [1229  289  858  274  883  714  479 1215  745  372] Cosine Similarity [0.20955687658197256, 0.20209478940976022, 0.16754548400656138, 0.1549189421369519, 0.12795573077159453, 0.12646652874780506, 0.1247497549876859, 0.12302934413093801, 0.12233109583693805, 0.12139542912762358]\n",
      "['Bibliographic control before and after MARC is reviewed.  The capability of keying into online systems brought an interdependence among libraries, the service centers that mediate between them, and the large utilities that process and distribute data.  From this has developed the basic network structure among libraries in the United States.  The independent development of major networks has brought problems in standardization and coordination. The authors point out that while technology has led toward centralization of automated library services, new developments are now pushing toward decentralization.  Coordination is a requirement to avoid fragmentation in this new environment.']\n",
      "Similar Doc Id [ 863 1344  280 1041  345 1170  869 1214  310  851] Cosine Similarity [0.18608647642230833, 0.14971883074146294, 0.13302793634880353, 0.13192041952528805, 0.12722025362498568, 0.1226688105461223, 0.12052913913223814, 0.11960402384484753, 0.11952653636734055, 0.11919897316755541]\n",
      "['The retrieval performance of book indexes can be measured in terms of their ability to direct a user selectively to text material whose identity but not location is known.  The method requires human searchers to base their searching strategies on actual passages from the book rather than on test queries, natural or contrived.  It circumvents the need for relevance judgement, but still yields performance indicators that correspond approximately to the recall and precision ratios of large document retrieval system evaluation.  A preliminary application of the method to the subject indexing of two major encyclopedias showed one encyclopedia apparently superior in both the finding and discrimination abilities of retrieval performance.  The method is presently best suited for comparative testing since its ability to yield absolute or reproducible measures is as yet not established.']\n",
      "Similar Doc Id [ 509  486   78 1194  482  178  564  491 1137  174] Cosine Similarity [0.18723357641547472, 0.16600382179920403, 0.1655755934178216, 0.16284960654520447, 0.16210664400768, 0.1454915437565848, 0.1450385838652664, 0.1449600570981761, 0.13706663119385373, 0.13442087262788072]\n",
      "[\"A linkage similarity measure which takes into account both the bibliographic coupling of documents and their cocitations (both cited and citing papers) produced improved document retrieval over a measure based only on bibliographic coupling.  The test collection consisted of 1712 papers whose relevance to specific queries had been judged by users.  To evaluate the effect of using cocitation data, we calculated for each query two measures of similarity between each relevant paper and every other paper retrieved. Papers were then sorted by the similarity measures, producing two ordered lists.  We then compared the resulting predictions of relevance, partial relevance, and non-relevance to the user's evaluations of the same papers. Overall, the change from the bibliographic coupling measure to the linkage similarity measure, representing the introduction of cocitation data, resulted in better retrieval performance.\"]\n",
      "Similar Doc Id [  73  753   69  486  291  992 1254 1281  622  893] Cosine Similarity [0.20729064229847094, 0.20484120028838948, 0.19957018656463618, 0.17086805127098084, 0.16869297237746742, 0.16703178538043706, 0.15459463107505902, 0.15129903223494912, 0.14494396345356467, 0.14445722989536436]\n",
      "['The way that individuals construct and modify search queries on a large interactive document retrieval system is subject to systematic biases similar to those that have been demonstrated in experiments on judgements under uncertainty.  These biases are shared by both naive and sophisticated subjects and cause the inquirer searching for documents on a large interactive system to construct and modify queries inefficiently.  A searching algorithm is suggested that helps the inquirer to avoid the effect of these biases.']\n",
      "Similar Doc Id [1412 1117  538 1162   70 1198  705 1072  502  626] Cosine Similarity [0.23350010994954684, 0.2076413856328136, 0.19630393509453128, 0.16902642136460969, 0.16489638068182627, 0.16033457124178035, 0.14096126768991682, 0.1319025651340877, 0.13059973901797733, 0.1292867857526006]\n",
      "['This article concerns the problem of how to permit a patron to represent the relative importance of various index terms in a Boolean request while retaining the desirable properties of a Boolean system. The character of classical Boolean systems is reviewed and related to the notion of fuzzy sets.  The fuzzy set concept then forms the basis of the concept of a fuzzy request in which weights are assigned to index terms. Ther properties of such a system are discussed, and it is shown that such systems retain the manipulability of traditional Boolean requests.']\n",
      "Similar Doc Id [ 809   53  738  318  659  772 1154 1362 1125  561] Cosine Similarity [0.2587069912319944, 0.18772185605152364, 0.1631679225518415, 0.16148762879494483, 0.14939710673798154, 0.13408653360229472, 0.13151518635718323, 0.12041708540498339, 0.11516971720782312, 0.11105562163883356]\n",
      "['A commercially available online search was used as a standard for comparative searching and evaluation of an in-house information system based on automatic indexing.  System features were identified and evaluated on the basis of their usefulness in various kinds of searching, their ease in implementation, and how they are influenced by differences in user type or specific applications.  Some common features of the commercial system, such as online instruction, user-specified print formats, dictionary display, and truncation, are seen to be unnecessary or impractical for the in-house system.  In designing the in-house system, therefore, detald consideration must be given to the applications, operating environment, and real user needs.  While a commercial system can serve as a useful standard for comparative evaluation, one must be careful not to attempt to duplicate it blindly in-house.']\n",
      "Similar Doc Id [ 912  501  377  869  592  616  940 1042  511  605] Cosine Similarity [0.16081373684694128, 0.15627676410374877, 0.1517004753718869, 0.14958180656898784, 0.14200915565521283, 0.13685954347400156, 0.13350517063993494, 0.12871010940446273, 0.1284498718585866, 0.12752553075147363]\n",
      "['It is argued that in information science we have to distinguish physical, objective, or document space from perspective, subjective, or information space.  These two spaces are like maps and landscapes: each is a systematic distortion of the other.  However, transformation can be easily made once the two spaces are distinguished.  If the transformations are omitted we only get unhelpful physical solutions to information problems.']\n",
      "Similar Doc Id [1022 1210  943  976  540 1351 1114 1118  987   86] Cosine Similarity [0.20151863368379047, 0.18939890520367425, 0.16626912197558993, 0.14371466946490663, 0.14306302852658573, 0.12588246657685925, 0.12473351296684593, 0.12281384306389459, 0.11170448662139501, 0.10712788839663022]\n",
      "['The use of document clusters has been suggested as an efficient file organization for a document retrieval system.  It is possible that by using this information about the relationships between documents that the effectiveness of the system (i.e., its ability to distinguish relevant from non-relevant documents) may also be improved.  In this paper a probabilistic model of cluster searching  based on query classification is described.  This model is tested with retrieval experiments which indicate that it can be more effective than heuristic cluster searches and cluster searches based on other models.  It can also be more effective than a full search in which every document is compared to the query.  The efficiency aspects of the implementation of the model are discussed.']\n",
      "Similar Doc Id [ 508  447  421  484  576  634 1312  575  565  569] Cosine Similarity [0.3357065691254722, 0.30529842710717864, 0.3039501893972597, 0.30388129882355663, 0.27720515596819373, 0.2551952647637084, 0.2506366129322632, 0.24765191908325823, 0.22962502538284935, 0.2067579594247064]\n",
      "['Current online library network technology is described, including the physical and functional aspects of networks.  Three types of networks are distinguished:  search service (e.g., SDC, Lockheed), customized service that provide bibliographic files (e.g., OCLC, Inc., RLIN), and service center (e.g., NELINET, INCOLSA).  It is predicted that as technology evolves more services will be provided outside the library directly to the user through his home or office.']\n",
      "Similar Doc Id [ 886 1366   91  739  155  946  997  118  173  213] Cosine Similarity [0.18554651024468902, 0.18348690383318905, 0.17698520759818692, 0.17355355976677578, 0.17322494414287143, 0.15961786927725286, 0.15780466264692616, 0.15774078195115074, 0.15675987226659957, 0.15034958944847274]\n",
      "['An experimental computer program has been developed to classify documents according to the 80 sections and five major section groupings of Chemical Abstracts (CA).  The program uses pattern recognition techniques supplemented by heuristics.  During the \"training\" phase, words from pre-classified documents are selected, and the probability of occurrence of each word in each section of CA is computed and stored in a reference dictionary.  The \"classification\" phase matches each word of a document title against the dictionary and assigns a section number to the document using weights derived from the probabilities in the dictionary.  Heuristic techniques are used to normalize word variants such as plurals, past tenses, and gerunds in both the training phase and the classification phase.  The dictionary lookup technique is supplemented by the analysis of chemical nomenclature terms into their component word roots to influence the section to which the documents are assigned.  Program performance and human consistency have been evaluated by comparing the program results against the published sections of CA and by conducting an experiment with people experienced in the assignment of documents to CA sections.  The program assigned approximately 78% of the documents to the correct major section groupings of CA and 67% of the correct sections or cross-references at a rate of 100 documents per second.']\n",
      "Similar Doc Id [ 829  152  252  726  676  740  701 1341  670  420] Cosine Similarity [0.19549023039825064, 0.16856141470820732, 0.16408867850401807, 0.15904764655421189, 0.14665441013991826, 0.14063288652706246, 0.13949195561912284, 0.13374061505492185, 0.13341235237591875, 0.12996183721095994]\n",
      "['Some of the automatic classification procedures used in information retrieval derive clusters of documents from an intermediate similarity matrix, the computation of which involves comparing each of the documents in the collection with all of the others.  It has recently been suggested that many of these comparisons, specifically those between documents having no terms in common, may be avoided by means of the uyse of an inverted file to the document collection.  This communication shows that the approach will effect reductions in the number of interdocument comparisons only if the documents are each indexed by a limited number of indexing terms; if exhaustive indexing is used, many document pairs will be compared several times over and the computation will be greater than when conventional approaches are used to generate the similarity matrix.']\n",
      "Similar Doc Id [ 569  361  678 1142 1071  327 1280   60  701  315] Cosine Similarity [0.2803137762460333, 0.26248926456901905, 0.22390899478579593, 0.13079524897735925, 0.1255007401589664, 0.12373097064624594, 0.12094296012592094, 0.11315335267756343, 0.11192629243105465, 0.1108270189124072]\n",
      "['The Use of a minicomputer in various phases of creating the thesaurus for the National Information Center for Special Education Materials (NICSEM) database is described.  The minicomputer is used to collect, edit, and correct candidate thesaurus terms.  The use of the minicomputer eases the process of grouping terms into files of similar concepts and facilitates the generation of products useful in vocabulary review and in term structuring.  Syndetic relations, indicated by assigning coded identification numbers, are altered easily in the design phase to reflect restructuring requirements.  Because thesaurus terms are already in machine- readable form, it is simple to prepare print programs to provide permuted, alphabetic, hierarchical, and chart formatted term displays.  Overall, the use of the minicomputer facilitates initial thesaurus entry development by reducing clerical effort, editorial staff decisions, and overall processing times.']\n",
      "Similar Doc Id [ 408  249 1154  152  353  182  510   49  724  173] Cosine Similarity [0.17929683963830023, 0.15379480659585332, 0.14335997626952093, 0.1343035682660191, 0.12944658176889234, 0.12801803504987724, 0.12557604846916243, 0.12468957659105712, 0.12456032611268159, 0.12329472787428455]\n",
      "['Decision Support Systems (DSS) represent a concept of the role of computers within the decision making process.  The term has become a rallying cry for researchers, practitioners, and managers concerned that Management Science and Management Information Systems fields have become unnecessarily narrow in focus.  As with many rallying cries, the term is not well defined.  For some writers, DSS simply mean interactive systems for use by managers.  To others, the key issue is support, rather than system.  They focus on understanding and improving the decision process; a DSS is then designed using any available and suitable technology. Some researchers view DSS as a subfield of MIS, while others regard it as an extension of Management Science techniques.  The former define Decision Support as providing managers with access to data and the latter as giving them access to analytic models.  The key argument of this paper is that the term DSS is relevant to situations where a \"final\" system can be developed only through an adaptive process of learning and evolution.  The design strategy must then focus on getting finished; this is very different from Management Science and Data Processing approaches.  The research issued for DSS center around adaption and evolution; they include managerial learning representation of tasks and user behavior, design architecture and strategies for getting started.']\n",
      "Similar Doc Id [ 878  636  377  372  893 1444 1142  571  293  134] Cosine Similarity [0.20461448984430664, 0.1625961578591273, 0.1599904453733661, 0.1517406326589159, 0.150105176717459, 0.1461056984819809, 0.1451353296882811, 0.14388649042084048, 0.13877242206245158, 0.13736489055419976]\n",
      "['A new method is described to extract significant phrases in the title and the abstreact of scientific or technical documents.  The method is based upon a text structure analysis and uses a relatively small dictionary. The dictionary has been constructed based on the knowledge about concepts in the field of science or technology and some lexical knowledge.  For significant phrases and their component items may be used in different meanings among the fields.  A text analysius approach has been applied to select significant phrases as substantial and semantic information carriers of the contents of the abstract.  The results of the experiment for five sets of documents have shown that the significant phrases are effectively extracted in all cases, and the number of them for every document and the processing time is fairly satisfactory.  The information representation of the document, partly using the method, is discussed with relation to the construction of the document information retrieval system.']\n",
      "Similar Doc Id [ 564   68  665 1280 1279    5  115 1075  482  660] Cosine Similarity [0.2727277785039483, 0.2675434525410606, 0.23515328380002096, 0.23031639080765065, 0.2299513938045068, 0.22825892812854467, 0.2133126450072986, 0.21232984617884204, 0.21117731072714688, 0.1940923873501538]\n",
      "['Passage retrieval (already operational for lawyers) has advantages in output form opver references retrieval and is economically feasible. Previous experiments in passage retrieval for scientists have demonstrated recall and false retrieval rates as good or better than those of present reference retrieval services.  The present experiment involved a greater variety of forms of retrieval question.  In addition, search words were selected independently by two different people for each retrieval question. The search words selected, in combination with the computer procedures used for passage retrieval, produced average recall ratios of 72 and 67%, respectively, for the two selectors.  The false retrieval rates were (except for one predictably difficult question) respectively 13 and 10 falsely retrieved sentences per answer-paper retrieved.']\n",
      "Similar Doc Id [494 635  67  59 323  83 417  85 316 318] Cosine Similarity [0.2555982231376938, 0.20912180744063943, 0.20848335089746156, 0.1923545955282545, 0.17526526956415062, 0.16923994562311437, 0.16680139926612475, 0.1662088024496607, 0.16571405430176722, 0.16545659365470405]\n",
      "['In this paper we describe a practical method of partial-match retrieval in very large data files.  A binary code word, called a descriptor, is associated with each record of the file.  These record descriptors are then used to form a derived descriptor for a block of several records, which will serve as an index for the block as a whole; hence, the name \"indexed descriptor files.\"  First the structure of these files is described and a simple, efficient retrieval algorithm is presented.  Then its expected behavior, in terms of storage accesses, is analyzed in detail.  Two different file creation procedures are sketched, and a number of ways in which the file organization can be \"tuned\" to a particular application is suggested.']\n",
      "Similar Doc Id [1117 1221  317 1123 1174  870  667  477 1170  834] Cosine Similarity [0.2918325398291733, 0.24842218792917156, 0.2368316948240587, 0.20342150815420953, 0.20193507160414645, 0.17868678864279605, 0.17348656058222758, 0.15829266173977383, 0.15538781093659926, 0.15143154539083817]\n",
      "['Recenty technological advances and the success of OCLC, Inc. has led to the emergence of three additional nonprofit library networks:  the Research Libraries Information Network (RLIN) of the Research Libraries Group, Inc., the University of Toronto Library Automation System (UTLAS), and the Washington Library Network (WLN).  This paper examines the economic and technological factors affecting the evolution of these networks and also explores the role of those state and regional (multistate) networks that broker OCLC services.  The competitive and cooperative nature of network relationships is a major theme of the discussion.']\n",
      "Similar Doc Id [ 886  118 1284  971  653  883  946   91  997  217] Cosine Similarity [0.17906858207732984, 0.17084107080108482, 0.16849888937102256, 0.16591361851908681, 0.16079002854583177, 0.159388778322151, 0.15667530476445526, 0.15080556413126184, 0.14387281639639096, 0.14365936909820223]\n",
      "['A new type of natural language parser is presented.  The idea behind this parser is to map input sentences into the deepest form of the representation of their meaning and inferences, as is appropriate.  The parser is not distinct from an entire understanding system.  It uses an integrated conception of inferences, scripts, plans and other knowledge to aid in the parse.  Furthermore, it does not attempt to parse everything it sees.  Rather, it determines what is most interesting and concentrates on that, ignoring the rest.']\n",
      "Similar Doc Id [1273 1295   25 1080  429 1135   94  882  426  526] Cosine Similarity [0.19014005222017444, 0.1549260315953025, 0.1480433529515796, 0.1455915055299361, 0.1413605573250752, 0.11097232823122598, 0.10844308496687967, 0.10811797075450805, 0.10498568847692716, 0.10189068842800811]\n",
      "['This paper discusses the origins of library networks and traces their development in the United States in the late 1960s through the present. The concept of resource sharing, with particular attention to the inter- library loan and programs for the cooperative acquisition and storage of materials, is examined in relationship to library networks.  In particular, attention is given to the question of how these two major components of library cooperation, which have tended to be separate, might become more closely integrated.']\n",
      "Similar Doc Id [ 393 1396 1389  960  549    9   46   81  927  589] Cosine Similarity [0.18059900936587114, 0.16983711338544752, 0.16206501668196563, 0.14122992196250025, 0.13623664014807227, 0.1326897830798655, 0.12906690370452545, 0.12571026679967068, 0.12367716850773441, 0.12106744923140418]\n",
      "['This paper presents a method of normalizations of English titles and their retrieval.  The title expressed by a noun phrase or a noun clause is converted to a function-expression by parsing.  For the retrieval with a reasonable recall rate as well as a high precision rate, the function- expression is transformed to a predicate-governor form, and then normalized to a standard form.  Therefrom, various items are extracted and recorded in a hierarchical tree-like inverted file.  In order to keep the recall rate in a reasonable value, several retrieval stages are implemented based on the key-term and case-label matching.  The retrieval is controlled by the preciseness of the specification of case-labels for each key-term.']\n",
      "Similar Doc Id [1130  867  868 1325  479  472 1140 1195  586 1243] Cosine Similarity [0.14716992487772368, 0.12729047357302747, 0.1219925141305009, 0.12137168834222024, 0.11945729559696881, 0.11852597582611152, 0.11647912999565727, 0.11392290932447291, 0.11222474964948298, 0.11215848073651749]\n",
      "[\"A generalization of the notion of ATN grammar, called a cascaded ATN (CATN), is prescribed.  CATN's permit a decomposition of complex language understanding behavior into a sequence of cooperating ATN's with separate domain of responsibility, where each stage (called an ATN transducer) takes its input from the output of the previous stage.  The paper includes an extensive discjussion of the principles of factoring-conceptual factoring reduces the number of places that a given fact needs to be represented in a grammar, and hypothesis factoring reduces the number of distinct hypotheses that have to be considered during parsing.\"]\n",
      "Similar Doc Id [ 396  395 1092  167 1128  147  123  576  659 1162] Cosine Similarity [0.18482843840712965, 0.17627310688763317, 0.1712038304126045, 0.14979044402515876, 0.1396733159680465, 0.13177371556291478, 0.11257080525731569, 0.11181629664485629, 0.10880905734217744, 0.10586178119450734]\n",
      "['Algorithms are given to process partially specified queries in a compressed database system.  The proposed methods handle effectively queries that use either whole words or word fragments as language elements. The methods are compared and critically evaluated in terms of the design and retrieval costs.  The analyses show that the method which exploits the interdependence of fragments as well as the relevance of fragments to records in the file has maximum design cost and least retrieval cost.']\n",
      "Similar Doc Id [ 614  859  874 1357  871  841 1365  619   71   73] Cosine Similarity [0.24247905815639778, 0.23937740741393568, 0.23118516562283334, 0.20707870807713671, 0.19029428275346943, 0.18079927155217776, 0.17559289209544715, 0.16939085910062623, 0.1678675566873131, 0.16431456120824817]\n",
      "[\"From the detailed analysis of eight previously published mathematical models, a general formulation of Bradford's distribution can be deduced as follows:  y = a log(x + c) + b, where y is the ratio of the cumulative frequency of articles to the total number of articles and x is the ratio of the rank of journals to the total number of journals.  The parameters a, b, and c are the slope, the intercept, and the shift in a straight line to log rank, respectively.  Each of the eight models is a special case of the general formulation and is one of five types of formulation.  In order to estimate three unknown parameters, a statistical method using root-weighted square error is proposed.  A comparative experiment using 11 databases suggests that the fifth type of formulation with three unknown parameters is the best fit to the observed data.  A further experiment shows that the deletion of the droop data leads to a more accurate value of parameters and less error.\"]\n",
      "Similar Doc Id [ 764  758  379   54   43  852  756 1220  805 1122] Cosine Similarity [0.22897854565111667, 0.18898329894956645, 0.16259237683128444, 0.15074077937023894, 0.1471318549785691, 0.14414645980494503, 0.14214769851770698, 0.13953345691201907, 0.12476686006401866, 0.11028469648318283]\n",
      "['The lexical problems in large information systems are created by the necessity of handling a great number of names and their interrelations. Such lexical problems are not covered completely by the concept data dictionaries, which are mostly concerned with database scheme design rather than the execution of operations.  In this paper we introduce our view of a lexical subsystem as a separate component in an information system architecture, to deal with linguistic and control functions concerning the lexical problems in local and network environments.  The lexical suybsystem is a special efficiently organized program package, which plays the role of a \"linguistic filter\" in a broad sense for lexically incorrect queries, promotes integration of databases and information retrieval systems, and facilitates the creation of local information systems.  We hope that lexical subsystems can become productive for any large, especially distributed, information system.']\n",
      "Similar Doc Id [ 878  873  331  954  778  458 1170 1010  997  231] Cosine Similarity [0.16133796437112635, 0.13852375774120856, 0.13561239723576876, 0.13551893871779422, 0.1328859956779372, 0.12958540669667776, 0.12778438153091487, 0.12669352767339473, 0.12625610745712101, 0.1243582644982032]\n",
      "['The relational model has received increasing attention during the past decade.  Its advantages include simplicity, consistency, and a sound theoretical basis.  In this article, the naturalness of viewing information retrieval relationally is demonstrated.  The relational model is presented, and the relational organization of a bibliographical database is shown. The notion of normalization is introduced and first, second, third, and fourth normal forms are demonstrated.  Relational languages are discussed, including the relational calculus, relational algebra, and SEQUEL. Numerous examples pertinent to information retrieval are presented in these relational languages.  Advantages of the relational approach to information retrieval are noted.']\n",
      "Similar Doc Id [  24 1140  828  839  604  653  309  571  514 1038] Cosine Similarity [0.20663466493525542, 0.14448418483402883, 0.132803314506672, 0.1315265570917534, 0.129945963600248, 0.12082576989213308, 0.1190830434833709, 0.11868464523575073, 0.11742391773811203, 0.11603293501596368]\n",
      "['This paper describes an architectural approach that provides information exchange across a broad spectrum of user applications and office automation offerings.  Some of the architectures described herein are currently implemented in existing IBM products.  These and other architectures will provide the basis for document interchange capability between products such as the IBM 5520 Administrative System, the IBM System/370 Distributed Office Support System (DISOSS), and the IBM Displaywriter System. Specifically described is a document distribution architecture and its associated data streams and others.  A general overview of the architectures as opposed to a detailed technical description is provided.  The architectures described are protocols for interchange between application processes; they do not address the specific user interface.  The document distribution architectures utilize SNA for data transmission and communications control facilities.']\n",
      "Similar Doc Id [ 833  992  592  616 1075 1326  421  308  850  691] Cosine Similarity [0.23448740152686853, 0.1864339155919581, 0.1860871245537846, 0.17833370858713213, 0.17115103111019075, 0.1704709899272735, 0.1598954991446112, 0.15440117306829054, 0.1436342323924234, 0.13782637078043253]\n",
      "[\"A technique is described for automatic reformulation of boolean queries.  Based on patron relevance judgements of an initial retrieval, prevalence measures are derived for terms appearing in the retrieved set of documents that reflect a term's distribution among the relevant and non-relevant documents.  These measures are then used to guide the construction of a boolean query for a subsequent retrieval.  To illustrate the technique, a series of tests is described of its application to a small data base in an experimental environment.  Results compare favourably with feedback as employed in a SMART-type system.  MOre extensive testing is suggested to validate the technique.\"]\n",
      "Similar Doc Id [ 607  809   53  318  738  511  482 1122 1326  772] Cosine Similarity [0.2320971158998769, 0.23123362205416845, 0.1970909981703763, 0.1809037745286115, 0.14957077559642704, 0.1265485388431626, 0.11558825530903546, 0.11022957644108723, 0.10221640810695042, 0.10022512892260516]\n",
      "['This paper is intended to propose a new methodological approach to the conception and development of natural language understanding systems. This new contribution is supported by the design, implementation, and experimentation of DONAU:  a general purpose domain oriented natural language understanding system developed and presently running at the Milan Polytechnic Artificial Intelligence Project.  The system is based on a two level modular architecture intended to overcome the lack of flexibility and generality often pointed out in many existing systems, and to facilitate the exchange of results and actual experiences between different projects. The horizontal level allows an independent and parallel development of the single segments of the system (syntactic analyser, information extractor, legality controller).  The vertical level ensures the possibility of changing (enlarging or redefining) the definition of the semantic domain on which each particular version of the system is oriented and specialized in a simple, incremental, and user-oriented way.  In the paper the general architecture of the system and the mode of operation of each segment are illustrated in detail.  Linguistic models, knowledge representation, and parsing algorithms are described and illustrated by means of selected examples.  Performance evaluations of the system in the application version on data base inquiry are reported and discussed.  Promising directions for future research are presented in the conclusions.']\n",
      "Similar Doc Id [1415  724  511  630   21 1201 1222  990  614  398] Cosine Similarity [0.1638297044381028, 0.14301648409960907, 0.13379713263144738, 0.13321180571014835, 0.13030866789199239, 0.1300674167620092, 0.1242814205097856, 0.12336221007479145, 0.11869153726171329, 0.11510537889786666]\n",
      "['Approximate matching of strings is reviewed with the aim of surveying techniques suitable for finding an item in a database when there may be a spelling mistake or other error in the keyword.  The methods found are classified as either equivalence or similarity problems. Equivalence problems are seen to be readily solved using canonical forms. For similarity problems difference measures are surveyed, with a full description of the well-established dynamic programming method relating this to the approach using probabilities and likelihoods.  Searches for approximate matches in large sets using a difference function are seen to be an open problem still, though several promising ideas have been suggested.  Approximate matching (error correction) during parsing is briefly reviewed.']\n",
      "Similar Doc Id [ 379 1220  491  298  749  586  408  725  768  514] Cosine Similarity [0.2184593871519921, 0.17713201591744848, 0.17071225667503614, 0.16864519195475786, 0.16278483070484595, 0.13529583106839568, 0.13069850898084565, 0.12986139057177098, 0.11592130416738464, 0.11016296330656632]\n",
      "['A prototype system is created that integrates a microfiche catalog into an online computer system for bibliographic control.  Costs and operational data are collected and analyzed.  The system permits the more economical microfiche storage of catalog records than would be feasible for comparable online magnetic disk storage.  Experimental tests demonstrate the feasibility of the online microfiche catalog system for use in library technical services and retrieval of bibliographic data.  The primary result of the project is the creation of a completely operational facility, including all equipment, software, procedures, and data bases necessary to demonstrate the system.  A second set of results is derived from the experimental use of the system and the evaluation of costs and times for various operations.  The cost effectiveness of the online microfiche catalog is demonstrated.']\n",
      "Similar Doc Id [ 298  873  855  644  949 1251   55 1438  264  208] Cosine Similarity [0.21233349024120604, 0.20598501051441231, 0.19957231026465463, 0.1939073245475122, 0.1914535390294028, 0.18933120905154482, 0.1885388997051118, 0.1882171450312376, 0.18718712517462247, 0.18647607047418838]\n",
      "['The question is asked whether it is feasible to use subsets of natural languages as query languages for data bases in actual applications using the question answering system \"USER SPECIALTY LANGUAGES\" (USL). Methods of evaluating a natural language based information system will be discussed.  The results (error and language structure evaluation) suggest how to form the general architecture of application systems which use a subset of German as query language.']\n",
      "Similar Doc Id [ 491  494  316  379  912  373  961 1444  318   28] Cosine Similarity [0.19343696368866944, 0.18871432942196772, 0.18783689864753933, 0.17564569972837302, 0.1589806426111317, 0.15677651920205007, 0.15020474757512267, 0.14454286654717485, 0.1427888489675314, 0.1393987060181893]\n",
      "[\"In 1978 Collier presented some hypothetical data on economic aspects of the use of online services as compared with subscriptions to printed services in libraries.  Collier's view of the economics of online searching seems misleadingly pessimistic because:  1.  It looks only at costs but not at effectiveness in comparing the two modes of access and searching.  An analysis combining cost and effectiveness aspects (i.e., a cost-effectiveness analysis) would give a completely different picture.  2.  The way the cost data are presented is grossly unfair to the online mode of access and use.  This work contains corrected information regarding online and printed services in libraries.\"]\n",
      "Similar Doc Id [1376   82  590  614  566 1365  821  291  522  489] Cosine Similarity [0.21365535036589917, 0.20308762203332684, 0.19564978795311977, 0.19081483040669725, 0.17871064279673968, 0.17749191375157872, 0.17544432853789993, 0.16847503856035, 0.1631911878517153, 0.15388676133528642]\n",
      "[\"Many information scientists are concerned with the operation of document retrieval systems serving scientists in various fields.  The scientists served by these systems are often members of what have been called invisible colleges, groups of scientists in frequent communication with one another and involved with highly specialized subject matters.  Often such groups are considered to share an intellectual perspective regarding this subject matter, which is sometimes referred to as a paradigm.  The purpose of this paper is to show how it is possible to identify paradigms, using the techniques of citation analysis.  I will operationalize the notion of paradigm as a 'consensual structure of concepts in a field.' Suppose we have obtained a set of papers pertaining to some topic.  Already knowing something about the field, we read each text and mark passages in which certain specific concepts are used or discussed.  For example, we might find that a concept designated 'A' appears in some sub-set of the papers.  Suppose further that we identify those papers in which concepts 'A' and 'B' are used together in the same papers in a certain specified manner. Clearly not all concepts will combine in a natural way, and not all authors combining concepts 'A' and 'B' will do so in the same way, though some predominant mode may emerge.  For a set of n concepts their structure is given by the totality of admissible combinations of concepts taken from two to n at a time.  The frequency with which a given combination occurs in the sample of papers on the topic is a measure of the degree of consensus regarding the particular concept combination within the corpus.  For concepts taken two at a time, the structure can be displayed as a graph with concepts as nodes and the relations between them represented as lines (arcs) connecting the nodes.  This definition of concept structure is similar to the semantic network of artificial intelligence except that in our approach a measure of consensus weights each arc of the graph.\"]\n",
      "Similar Doc Id [ 971  106 1421 1309  217  569 1178   41  640 1223] Cosine Similarity [0.20096837894472194, 0.1614855771923542, 0.16003422072100057, 0.15835035792158883, 0.14668082099271737, 0.1451715449876556, 0.1416837651938554, 0.12876153654717362, 0.12856513107660503, 0.12838323466481832]\n",
      "['One mode of online retrieval in Scisearch or Social Scisearch involves entering pairs of authors\\' names believed to be jointly cited by subsequent writers and retrieving papers in which cocitations occur.  Six pairs were formed with the names of four authors prominent in the social indicators movement (Bauer, Duncan, Land, and Sheldon).  Documents by the four were not specified.  It was thought that the pair Duncan and Land would retrieve papers in which indicator-type data would be integrated with path-analytic causal modeling.  All other pairs seemed likely to retrieve a \"general social indicators\" literature.  The 298 retrieved papers confirmed expectattions.  It was found that 121 papers generally cited social indicators (SI) documents by the input authors and frequently had SI language in their titles.  Other signs of content also identified them as papers of the SI movement.  The 177 papers retrieved on Duncan and Land generally cited causal modeling documents by the input pair and were path-analytic in nature.  As expected, they were relatively \"harder\" than the first group of papers, although the two groups are akin and are formally linked through citations in certain papers.  An additional result is that papers citing at least three of the input authors tend to be overviews of the SI movement.']\n",
      "Similar Doc Id [ 680 1344   92  467  214 1458  307   25  778  664] Cosine Similarity [0.1493407766424697, 0.12723761362527913, 0.09641795881739784, 0.09379033669443639, 0.09147602117455764, 0.09106582618144944, 0.08804587187610746, 0.08116877645444656, 0.08063034027965364, 0.07647728041555203]\n",
      "['The number of databases, records contained in databases and the online use of databases has increased dramatically over the past several years, bringing the 1979 totals for bibliographic, bioliographic-related, and natural language databases to 528.  These 528 databases contain 148 million records.  Some 4 million online searches were conducted via the major U.S. and Canadian systems in 1979.']\n",
      "Similar Doc Id [ 874  920  370   40  698  242 1196  516  618  558] Cosine Similarity [0.21144768675039802, 0.19543075740941435, 0.19530682300330168, 0.18799678826553895, 0.15933633235836528, 0.14514890537605238, 0.13629288710969426, 0.13604157052213556, 0.13014879605422125, 0.12273428330658204]\n",
      "['A method of iterative searching, using the results of one iteration search to formulate the next iteration search, was applied to a full-text database consisting of some 2400 documents and 1,3000,000 text-words of Hebrew and Aramaic.  The iterative method consists of clustering the documents returned in an iteration, using weighting by proximity and by frequency simultaneously. The process produces searchonyms, which are terms synonymous to keywords in the context of a single query.  Augumenting or replacing keywords by searchonyms via manual or automatic feedback leads to the formulation of the next iteration search.  The results of the experiment are consistent with those of an earlier small-scale experiment on an English database, and indicate that in contrast to global clustering where the size of matrices limits applications to small databases and improvements are doubtful, local metrical methods appear to be well suited to arbitrarily large databases, improving precision and recall simultaneously.  Further experiments using more test-queries run on even larger databases should be made to collect further evidence as to the performance of these methods.']\n",
      "Similar Doc Id [1279  482   71  174  316  178  681  607  564  509] Cosine Similarity [0.2120782784417166, 0.20000568695189763, 0.16328467671167224, 0.16024018611527388, 0.1547628722307437, 0.15318111958123928, 0.14687756064993124, 0.14677384507056057, 0.1386189776969534, 0.1338532625972874]\n",
      "['REFLES is a microcomputer-based system for data retrieval in library environments.  The problem of information retrieval is discussed from a theoretical point of view, followed by an analysis of the reference process and data thereby gathered, leading to a description of REFLES in terms of its hardware and software.  REFLES, a prototype system at present, currently functions in a test environment.  Examples of data contained in the system and of its use are presented.  Future considerations and speculations on other versions of the system conclude the paper.']\n",
      "Similar Doc Id [ 421  552  373  814 1160  603 1375 1095  735 1153] Cosine Similarity [0.22131393807858907, 0.21735209802536154, 0.19585402503776198, 0.1930849671870878, 0.18993321977256236, 0.18991094980115528, 0.1873108112230161, 0.18562160159760221, 0.18381014923850833, 0.17636189804281868]\n",
      "['A major deficiency of traditional Boolean systems is their inability to represent the varying degrees to which a document may be written on a subject. In this article we isolate a number of criteria that should be met by any Boolean system generalized to have a weighting capability.  It is proven that only one weighting rule satisfies these conditions--that associated with fuzzy- set theory--and that this weighting scheme satisfies most of the other properties associated with Boolean algebra as well.  Probabilistic weighting is then introduced as an alternative approach and the two systems compared. In the limit of zero/one weights, all systems considered converge to traditional Boolean retrieval.']\n",
      "Similar Doc Id [ 738  809   53  318   24  522  772  837 1140 1229] Cosine Similarity [0.2547015704778119, 0.24752866214701552, 0.20633200519865202, 0.17821414933521754, 0.1460175437894124, 0.1383875936011005, 0.10644022873526036, 0.10448024077798618, 0.09843616147326462, 0.09716810201567534]\n",
      "['Several papers have appeared that have analyzed recent developments in the problem of processing, in a document retrieval system, queries expressed as Boolean expressions.  The purpose of this paper is to continue that analysis. We shall show that the concept of threshold values resolves the problems inherent with relevance weights.  Moreover, we shall explore possible evaluation mechanisms for retrieval of documents, based on fuzzy-set-theoretic considerations.']\n",
      "Similar Doc Id [ 569 1284  824  459  318  809   53 1335  964 1050] Cosine Similarity [0.1972329712788463, 0.19253428017552005, 0.1705813216091676, 0.15114900612248203, 0.1482802315220174, 0.146284003911902, 0.1455853443152951, 0.13579501638053698, 0.1338775533040001, 0.13224024763060022]\n",
      "['There has been a good deal of work on information retrieval systems that have continuous weights assigned to the index terms that describe the records in the database, and/or to the query terms that describe the user queries. Recent articles have analyzed retrieval systems with continuous weights of either type and/or with a Boolean structure for the queries.  They have also suggested criteria which such systems ought to satisfy and record evaluation mechanisms which partially satisfy these criteria.  We offer a more careful analysis, based on a generalization of the discrete weights.  We also look at the weights from an entirely different approach involving thresholds, and we generate an improved evaluation mechanism which seems to fulfill a larger subset of the desired criteria than previous mechanisms.  This new mechanism allows the user to attach a \"threshold\" to the query term.']\n",
      "Similar Doc Id [1100 1375  569  193  809  499  869   49 1374 1214] Cosine Similarity [0.15448401807163017, 0.14763273632958082, 0.13614855133052084, 0.13291353227391237, 0.13218937445422924, 0.13199523299278643, 0.13177848876358023, 0.13097587138240194, 0.12678984297933316, 0.1244110132665723]\n",
      "['Online retrieval systems may be difficult to use, especially by end users, because of heterogeneity and complexity.  Investigations have concerned the concept of a translating computer interface as a means to simplify access to, and operation of, heterogeneous bibliographic retrieval systems and databases.  The interface allows users to make requests in a common language. These requests are translated by the interface into the appropriate commands for whatever system is being interrogated.  System responses may also be transformed by the interface into a common form before being given to the users.  Thus, the network of different systems is made to look like a single \"virtual\" system to the user.  The interface also provides instruction and other search aids for the user.  The philosophy, design, and implementation of an experimental interface named CONIT are described.']\n",
      "Similar Doc Id [ 293  507  971  541  874 1256  653 1306  377  213] Cosine Similarity [0.2111036255724791, 0.16397833470575113, 0.1623310746183613, 0.1549260589995105, 0.15108353344302478, 0.15047273268344286, 0.14354530863134, 0.14146889198697724, 0.13992055414930568, 0.13933896914763788]\n",
      "['The evaluation of the concept of a translating compuyter interface for simplifying operation of multiple, heterogenous online bibliographic retrieval systems has been undertaken.  An experimental retrieval system, named CONIT, was built and tested under controlled conditions with inexperienced end users.  A detailed analysis of the experimental usages showed that users were able to master interface operation sufficiently well to find relevant document references.  Success was attributed, in part, to a simple command language, adequate online instruction, and a simplified natural-language, keyword/stem approach to searching.  It is concluded that operational interfaces of the type studied can provide for increased usability of existing system in a cost effective manner, especially for searchers. Furthermore, more advanced interfaces based on improved instruction and automated search strategy techniques could further enhance retrieval effectiveness for a wide class of users.']\n",
      "Similar Doc Id [467 477 855  85 837  77 801 895 501 768] Cosine Similarity [0.17074136268953316, 0.14847083074250564, 0.13972460146792978, 0.1344227366363894, 0.12702901269418243, 0.12515143070381501, 0.12367976460389012, 0.12090184403697724, 0.11873500052848537, 0.11843459421440546]\n",
      "['This paper notes the benefits accruing from interaction between computerized retrieval systems and micrographic retrieval systems.  It reviews current state of automated micrographic retrieval technology.  The conclusion is that with a combination of advances in communications technology, and sophisticated indexing input from libraries and information scientists, the new generation of automated micrographs devices may constitute the on-line document retrieval systems of the future.']\n",
      "Similar Doc Id [ 885  808  445  331 1103  631 1075  343  696  883] Cosine Similarity [0.1877629777852392, 0.1784127884860821, 0.1710888724267832, 0.16393297310336985, 0.16103737772270765, 0.16021387760731384, 0.15719369573366646, 0.15259444291660465, 0.15006132217876295, 0.14920222046563614]\n",
      "['Conventional information retrieval processes are largely based on data movement, pointer manipulations and integer arithmetic; more refined retrieval algorithms may in addition benefit from substantial computational power.  In the present study a number of parallel processing methods are described that serve to enhance retrieval services.  In conventional retrieval environments parallel list processing and parallel search facilities are of greatest interest.  In more advanced systems, the use of array processors also proves beneficial.  Various information retrieval processes are examined and evidence is given to demonstrate the usefulness of parallel processing and fast computational facilities in information retrieval.']\n",
      "Similar Doc Id [1088  139  942 1203  513  852 1369 1171 1338 1365] Cosine Similarity [0.17574143643410203, 0.12978184023071782, 0.122934276365897, 0.11959782514066908, 0.11119532437708744, 0.10688733775461218, 0.1018658311141189, 0.09786010562685027, 0.09634948777597659, 0.09616485635404212]\n",
      "['The frequency characteristics of terms in the documents of a collection have been used as indicators of term importance for content analysis and indexing purposes.  In particular, very rare or very frequent terms are normally believed to be less effective than medium-frequency terms.  Recently automatic indexing theories have been devised that use not only the term frequency characteristics but also the relevance properties of the terms. The major term-weighting theories are first briefly reviewed.  The term precision and term utility weights that are based on the occurrence characteristics of the terms in the relevant, as opposed to the nonrelevant, documents of a collection are then introduced.  Methods are suggested for estimating the relevance properties of the terms based on their overall occurrence characteristics in the collection.  Finally, experimental evaluation results are shown comparing the weighting systems using the term relevance properties with the more conventional frequency-based methodologies.']\n",
      "Similar Doc Id [ 811  823 1325  763  804 1306 1242   43  389  988] Cosine Similarity [0.28791831787143146, 0.19576342816476722, 0.19448356316620613, 0.18226375783152415, 0.1695425714401659, 0.15899120669152814, 0.15253773188198771, 0.1493477537338827, 0.1486132253807037, 0.1473424752377523]\n",
      "['This paper describes the design and implementation of an \"electronic filing machine,\" a machine which is capable of storing large numbers of \"unstructured\" documents in such a way a particular document may be easily and quickly retrieved.  A functional distributed architecture permits the implementation of the system in a mixture of hardware and software.']\n",
      "Similar Doc Id [ 753  293 1026  631  552  661   57  387 1075  992] Cosine Similarity [0.23614414466726927, 0.18490799308510053, 0.17826501924899485, 0.17178107743755167, 0.1715239745089814, 0.1588667542847109, 0.14692717990521983, 0.14452493453105617, 0.14425909620337585, 0.14376744861945556]\n",
      "['This paper tackles the problem of how one might select further search terms, using relevance feedback, given the search terms in the query.  These search terms are extracted from a maximum spanning tree connecting all the terms in the index term vocabulary.  A number of different spanning trees are generated from a variety of association measures.  The retrieval effectiveness for the different spanning trees is shown to be approximately the same.  Effectiveness is measured in terms of precision and recall, and the retrieval tests are done on three different test collections.']\n",
      "Similar Doc Id [  56  602  809 1279  659  509  775 1229   85  594] Cosine Similarity [0.20566969234912322, 0.20543504566317627, 0.1995586032043542, 0.1980915082555436, 0.18813370702255566, 0.17922924475094373, 0.17864284087027418, 0.17001197913171992, 0.16032455114495464, 0.15698441963598633]\n",
      "['Indexing quality determines whether the information content of an indexed document is accurately represented.  Indexing effectiveness measures whether an indexed document is correctly retrieved every time it is relevant to a query.  Measurement of these criteria is cumbersome and costly; data base producers therefore prefer inter-indexer consistency as a measure of indexing quality or effectiveness.  The present article assesses the validity of this substitution in various environments.']\n",
      "Similar Doc Id [1375  992 1075 1164  804  564  223  502  726 1129] Cosine Similarity [0.32790824591086454, 0.1724086494430308, 0.17095215122305193, 0.16366578452559172, 0.15824874253651833, 0.15183338063743926, 0.14969299835828845, 0.14515103354388284, 0.14281724366996035, 0.14218169775645745]\n",
      "[\"A set of experiments was conducted to determine the suitability of the Colon Classification as a foundation for the automated analysis, representation and retrieval of primary information from the full text of documents.  Primary information is that information embodied in the text of a document, as opposed to secondary information which is generally in such forms as:  an abstract, a table of contents, or an index. Full text databases were created in two subject areas and queries solicited from specialists in each area.  An automated full text indexing system, along with four automated passage retrieval systems, was created to test the various features of the Colon Classification.  Two Boolean-based systems and one simple word occurrence system were created in order to compare the retrieval results against types of systems which are in more common use.  The systems' retrieval performances were measured using recall and precision and the mean expected search length reduction factors. Overall, it was found that the Colon Classification-based systems did not perform significantly better than the other systems.\"]\n",
      "Similar Doc Id [1279  509  809  806  328  526  522  482  564   50] Cosine Similarity [0.23240460639830318, 0.2090683673126055, 0.19795222450498795, 0.18277874091894433, 0.17592811264543887, 0.17357762096910206, 0.17217534428700043, 0.17053037689500292, 0.16673622616782852, 0.16061885440691917]\n",
      "['A study was carried out of the relationship between the vocabulary of user queries and the vocabulary of documents relevant to the queries, and the value of adding to the document description record in a retrieval system keywords from previous queries for which the document had proved useful. Two test databases incorporating user query keywords were implemented at the School of Library and Information Science, University of Western Ontario.  Clustering of the documents via title and user keywords, a statistical analysis of title-user keyword co-occurrences, and retrieval tests were used to examine the effect of the added keywords.  Results showed the impracticality of the procedure in an operational setting, but indicated the value of analyses with sample data in the development and maintenance of keyword dictionaries and thesauri.']\n",
      "Similar Doc Id [ 801   85  595   33  509 1117  992  487  768   48] Cosine Similarity [0.22636402324363175, 0.2188027195632581, 0.21327310972734065, 0.18909550648150877, 0.17753282905931048, 0.17194424898556115, 0.166185250930823, 0.16479771663412254, 0.1643937015903122, 0.16405204451499672]\n",
      "[\"A technique of online instruction and assistance to bibliographic data base searchers called Individualized Instruction for Data Access (IIDA) is being developed by Drexel University.  IIDA assists searchers by providing feedback based on real-time analysis while searches are being performed. Extensive help facilities which draw on this analysis are available to users.  Much of the project's experimental work, as described elsewhere, is concerned with the process of searching and the behavior of searchers. This paper will largely address itself to the project's computer system, which is being developed by subcontract with the Franklin Institute's Science Information Services.\"]\n",
      "Similar Doc Id [1206 1095  566  147   40  881 1375 1077  422 1066] Cosine Similarity [0.21690715571392732, 0.21101057654151154, 0.18325798709233462, 0.1556585843607576, 0.15022606913408437, 0.14855011345333688, 0.14831960142030184, 0.13942227268581425, 0.13672310046327296, 0.1349419985036467]\n",
      "['It is shown that the mapping of a particular area of science, in this case information science, can be done using authors as units of analysis and the cocitations of pairs of authors as the variable that indicates their \"distances\" from each other.  The analysis assumes that the more two authors are cited together, the closer the relationship between them.  The raw data are cocitation counts drawn online from Social Scisearch (Social Sciences Citation Index) over the period 1972-1979.  GThe resulting map shows (1) identifiable author groups (akin to \"schools\") of information science, (2) locations of these groups with respect to each other, (3) the degree of centrality and peripherality of authors within groups, (4) proximities of authors within group and across group boundaries (\"border authors\" who seem to connect various areas of research), and (5) positions of authors with respect to the map\\'s axes, which were arbitrarily set spanning the most divergent groups in order to aid interpretation.  Cocitation analysis of authors offers a new technique that might contribute to the understanding of intellectual structure in the sciences and possibly in other areas to the extent that those areas rely on serial publications.  The technique establishes authors, as well as documents, as an effective unit in analyzing subject specialties.']\n",
      "Similar Doc Id [1273  429   25  104 1139  521  637  877 1431   94] Cosine Similarity [0.16516027126065158, 0.16235203968397177, 0.14940240573236854, 0.1445334321403314, 0.12637655492491384, 0.11966268559131532, 0.11660159824814834, 0.1048183728989526, 0.10413211671030317, 0.10388010785082945]\n",
      "['The \"Office of the Future,\" \"Office Technology,\" \"Word Processing,\" \"Electronic Mail,\" \"Electronic Communications,\" \"Convergence,\" \"Information Management.\"  These are all terms included in the current list of buzz words used to describe current activities in the office technology area.  The high level of investment in factories and plants and the ever-increasing fight to improve productivity by automating the dull, routine jobs are usually quoted and compared with the extremely low investment in improving and automating the equally tedious routine jobs in the office environment; the investment in the factory is quoted as being ten times greater per employee than in the office.  This, however, is changing rapidly and investment on a large scale is already taking place in manhy areas as present-day inflation bites hard, forcing many companies and organizations to take a much closer look at their office operations.']\n",
      "Similar Doc Id [ 690  827  756 1103  719 1055  716 1361  820  516] Cosine Similarity [0.12662111524400765, 0.11982754367275836, 0.1167020871053012, 0.1161203102783891, 0.11098565774513701, 0.10688943457575566, 0.09889251691383126, 0.09182718367099177, 0.09000774366760103, 0.08917555133196008]\n",
      "['An automated document clustering procedure is described which does not require the use of an inter-document similarity matrix and which is independent of the order in which the documents are processed.  The procedure makes use of an initial set of clusters which is derived from certain of the terms in the indexing vocabulary used to characterise the documents in the file.  The retrieval effectiveness obtained using the clustered file is compared with that obtained from serial searching and from use of the single-linkage clustering method.']\n",
      "Similar Doc Id [569 361 920 317  61 992 678  50 619 877] Cosine Similarity [0.21981960062127542, 0.20027200888593488, 0.18149086351090038, 0.17502294849345815, 0.1585726836593355, 0.15787825362582986, 0.15648477835263583, 0.1517625668674022, 0.15175504782450736, 0.14944976886624833]\n",
      "['A fast algorithm is described for comparing the lists of terms representing documents in automatic classification experiments.  The speed of the procedure arises from the fact that all of the non-zero-valued coefficicents for a given document are identified together, using an inverted file to the terms in the document collection.  The complexity and running time of the algorithm are compared with previously described procedures.']\n",
      "Similar Doc Id [ 852 1418  317  502  449 1117  753 1373 1075  568] Cosine Similarity [0.2444399152345768, 0.17444514235607192, 0.1669809377128703, 0.16443230470926806, 0.1586603347565253, 0.15301633913815033, 0.14743544528091287, 0.1392120273649937, 0.12756259341499432, 0.12609051413355976]\n"
     ]
    }
   ],
   "source": [
    "for key in qry_set.keys():\n",
    "\n",
    "    user_question = []\n",
    "    user_question.append(qry_set[key])\n",
    "    print(user_question)\n",
    "    sim_vecs, cosine_similarities = calculate_similarity(X_tf, tfidf_vectorizor, user_question)\n",
    "    print('Similar Doc Id',sim_vecs,'Cosine Similarity',[cosine_similarities[i] for i in sim_vecs])\n",
    "    #print(rel_set[key],'\\n----------------------')\n",
    "    #show_similar_documents(data, cosine_similarities, sim_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Identify terms in these documents which are distinctive to the documents.(2Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of document10\n",
      " The Acquisition of Library Materials Ford, S. The scope of acquisitions work, outlined in the Introduction, acknowledges the importance of selection policy, serials recording, and other topics kindred to acquisitions. These topics are discussed in this book only as they relate to obtaining library materials.  They are examined thoroughly in books and papers that are cited in the references and the bibliographic note. Centralized acquisitions and automation of order routines are of major importance in order work and they are reviewed as chapters in this book.  These chapters are introductions to the concepts and problems of centralization and automation, not manuals of practice. For treatment of these topics in particular and in depth the reader is referred to the references cited.  For automation these references are only a modest selection from an enormous literature.  \n",
      "Top 20 keyword of document 10\n",
      " {'ford': 0.518, 'modest': 0.426, 'depth': 0.338, 'treatment': 0.332, 'scope': 0.269, 'note': 0.266, 'order': 0.26, 'reader': 0.17, 'work': 0.158, 'particular': 0.155, 'book': 0.14, 'major': 0.135} \n",
      "Unique words in document\n",
      " {'automation,', 'Acquisition', 'obtaining', 'cited.', 'book.', 'is', 'reviewed', 'selection', 'topics', 'as', 'these', 'Materials', 'problems', 'an', 'Centralized', 'S.', 'references', 'thoroughly', 'These', 'relate', 'enormous', 'automation', 'The', 'library', 'outlined', 'to', 'concepts', 'kindred', 'work,', 'they', 'are', 'of', 'modest', 'routines', 'acquisitions', 'acknowledges', 'chapters', 'recording,', 'other', 'Introduction,', 'treatment', 'Library', 'reader', 'book', 'from', 'in', 'cited', 'discussed', 'this', 'practice.', 'work', 'particular', 'They', 'importance', 'papers', 'major', 'not', 'serials', 'policy,', 'referred', 'note.', 'introductions', 'only', 'order', 'materials.', 'Ford,', 'centralization', 'acquisitions.', 'that', 'examined', 'and', 'manuals', 'a', 'scope', 'depth', 'the', 'bibliographic', 'books', 'literature.', 'For'}\n"
     ]
    }
   ],
   "source": [
    "idx=10\n",
    "print(f'Content of document{idx}\\n',data['text'][idx],f'\\nTop 20 keyword of document {idx}\\n',data['keywords'][idx],'\\nUnique words in document\\n',data['unique_word'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f134e",
   "metadata": {},
   "source": [
    "#### Getting Result from Query and showing the Mean Reciprocal Rank to overall queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_from_query(qry_id,qry_set,bm25,process_flag=False,distinct_flag=False,distinct_terms=\"\"):\n",
    "    \"\"\"Return an ordered array of relevant documents returned by query_id\n",
    "\n",
    "    Args:\n",
    "        qry_id (int): id of query on dataset\n",
    "        bm25 (object): indexed corpus\n",
    "\n",
    "    Returns:\n",
    "        boolean sorted relevance array of documents\n",
    "    \"\"\"    \n",
    "    query = qry_set[qry_id]\n",
    "    rel_docs = []\n",
    "    if distinct_flag:\n",
    "        query = query + distinct_terms\n",
    "    if qry_id in rel_set:\n",
    "        rel_docs = rel_set[qry_id]\n",
    "    if process_flag:\n",
    "        tokenized_query = preprocess_string(query, True, True, True)\n",
    "    else:\n",
    "        tokenized_query = query.split(\" \")\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    most_relevant_documents = np.argsort(-scores)\n",
    "    masked_relevance_results = np.zeros(most_relevant_documents.shape)\n",
    "  \n",
    "    masked_relevance_results[rel_docs] = 1\n",
    "    sorted_masked_relevance_results = np.take(masked_relevance_results, most_relevant_documents)\n",
    "    \n",
    "    return sorted_masked_relevance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "def preprocess_string(txt, remove_stop=True, do_stem=True, to_lower=True):\n",
    "    \"\"\"\n",
    "    Return a preprocessed tokenized text.\n",
    "    \n",
    "    Args:\n",
    "        txt (str): original text to process\n",
    "        remove_stop (boolean): to remove or not stop words (common words)\n",
    "        do_stem (boolean): to do or not stemming (suffixes and prefixes removal)\n",
    "        to_lower (boolean): remove or not capital letters.\n",
    "        \n",
    "    Returns:\n",
    "        Return a preprocessed tokenized text.\n",
    "    \"\"\"    \n",
    "    if to_lower:\n",
    "        txt = txt.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(txt)\n",
    "    \n",
    "    if remove_stop:\n",
    "        tokens = [tk for tk in tokens if tk not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(tk) for tk in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c956962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Tokenization: MRR@10 0.3146\n",
      "With Tokenization MRR@10 0.4187\n"
     ]
    }
   ],
   "source": [
    "corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(corpus)\n",
    "results = [results_from_query(qry_id, qry_set,bm25) for qry_id in list(qry_set.keys())]\n",
    "print('Without Tokenization: MRR@10 %.4f' % mean_reciprocal_rank(results))\n",
    "\n",
    "corpus = list(doc_set.values())\n",
    "tokenized_corpus = [preprocess_string(doc, remove_stop=True, do_stem=True, to_lower=True) for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "results = [results_from_query(qry_id, qry_set,bm25,process_flag=True) for qry_id in list(qry_set.keys())]\n",
    "print('With Tokenization MRR@10 %.4f' % mean_reciprocal_rank(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Add the terms to the query, and re-run the retrieval process. Return the final results.(2Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17659055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding ford to the query\n",
      "With Tokenization and disinct term MRR@10 0.4238 \n",
      "\n",
      "Adding modest to the query\n",
      "With Tokenization and disinct term MRR@10 0.4257 \n",
      "\n",
      "Adding depth to the query\n",
      "With Tokenization and disinct term MRR@10 0.4257 \n",
      "\n",
      "Adding treatment to the query\n",
      "With Tokenization and disinct term MRR@10 0.4222 \n",
      "\n",
      "Adding scope to the query\n",
      "With Tokenization and disinct term MRR@10 0.4168 \n",
      "\n",
      "Adding note to the query\n",
      "With Tokenization and disinct term MRR@10 0.4236 \n",
      "\n",
      "Adding order to the query\n",
      "With Tokenization and disinct term MRR@10 0.4098 \n",
      "\n",
      "Adding reader to the query\n",
      "With Tokenization and disinct term MRR@10 0.4259 \n",
      "\n",
      "Adding work to the query\n",
      "With Tokenization and disinct term MRR@10 0.4133 \n",
      "\n",
      "Adding particular to the query\n",
      "With Tokenization and disinct term MRR@10 0.4188 \n",
      "\n",
      "Adding book to the query\n",
      "With Tokenization and disinct term MRR@10 0.4257 \n",
      "\n",
      "Adding major to the query\n",
      "With Tokenization and disinct term MRR@10 0.4135 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx= 10\n",
    "for keys in data['keywords'][idx]:\n",
    "    print(f'Adding {keys} to the query')\n",
    "    results = [results_from_query(qry_id, qry_set,bm25,process_flag=True,distinct_flag=True,distinct_terms=keys) for qry_id in list(qry_set.keys())]\n",
    "    print('With Tokenization and disinct term MRR@10 %.4f \\n' % mean_reciprocal_rank(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_MIN_LENGTH = 2 ## we'll drop all tokens with less than this size\n",
    "\n",
    "def strip_accents(text):\n",
    "    \"\"\"Strip accents and punctuation from text. \n",
    "    For instance: strip_accents(\"João e Maria, não entrem!\") \n",
    "    will return \"Joao e Maria  nao entrem \"\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "    str: text without accents and punctuation\n",
    "\n",
    "   \"\"\"    \n",
    "    nfkd = unicodedata.normalize('NFKD', text)\n",
    "    newText = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "    return re.sub('[^a-zA-Z0-9 \\\\\\']', ' ', newText)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Make all necessary preprocessing of text: strip accents and punctuation, \n",
    "    remove \\n, tokenize our text, convert to lower case, remove stop words and \n",
    "    words with less than 2 chars.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "    str: cleaned tokenized text\n",
    "\n",
    "   \"\"\"        \n",
    "    text = strip_accents(text)\n",
    "    text = re.sub(re.compile('\\n'),' ',text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in STOP_WORDS and len(word) >= WORD_MIN_LENGTH]\n",
    "    return words\n",
    "    \n",
    "def inverted_index(words):\n",
    "    \"\"\"Create a inverted index of words (tokens or terms) from a list of terms\n",
    "\n",
    "    Parameters:\n",
    "    words (list of str): tokenized document text\n",
    "\n",
    "    Returns:\n",
    "    Inverted index of document (dict)\n",
    "\n",
    "   \"\"\"       \n",
    "    inverted = {}\n",
    "    for index, word in enumerate(words):\n",
    "        locations = inverted.setdefault(word, [])\n",
    "        locations.append(index)\n",
    "    return inverted\n",
    "\n",
    "def inverted_index_add(inverted, doc_id, doc_index):\n",
    "    \"\"\"Insert document id into Inverted Index\n",
    "\n",
    "    Parameters:\n",
    "    inverted (dict): Inverted Index\n",
    "    doc_id (int): Id of document been added\n",
    "    doc_index (dict): Inverted Index of a specific document.\n",
    "\n",
    "    Returns:\n",
    "    Inverted index of document (dict)\n",
    "\n",
    "   \"\"\"        \n",
    "    for word in doc_index.keys():\n",
    "        locations = doc_index[word]\n",
    "        indices = inverted.setdefault(word, {})\n",
    "        indices[doc_id] = locations\n",
    "    return inverted\n",
    "\n",
    "## Using AND as logical operator\n",
    "def boolean_search(inverted, file_names, query):\n",
    "    \"\"\"Run a boolean search with AND operator between terms over \n",
    "    the inverted index.\n",
    "\n",
    "    Parameters:\n",
    "    inverted (dict): Inverted Index\n",
    "    file_names (list): List with names of files (books)\n",
    "    query (txt): Query text\n",
    "\n",
    "    Returns:\n",
    "    Names of books that matchs the query.\n",
    "\n",
    "   \"\"\"      \n",
    "    # preprocess the user query using same function used to build Inverted Index\n",
    "    words = [word for _, word in enumerate(tokenize_text(query)) if word in inverted]\n",
    "    # list with a disctinct document match for each term from query\n",
    "    results = [set(inverted[word].keys()) for word in words]\n",
    "    # AND operator. Replace & for | to modify to OR behavior.\n",
    "    docs = reduce(lambda x, y: x & y, results) if results else []\n",
    "    return ([file_names[doc] for doc in docs])\n",
    "\n",
    "def ranked_search(k, tf_idf_index, file_names, query,tf_idf):\n",
    "    \"\"\"Run ranked query search using tf-idf model.\n",
    "\n",
    "    Parameters:\n",
    "    k (int): number of results to return\n",
    "    tf_idf_index (dict): Data Structure storing Tf-Idf weights to each \n",
    "                        pair of (term,doc_id) \n",
    "    file_names (list): List with names of files (books)\n",
    "    query (txt): Query text\n",
    "\n",
    "    Returns:\n",
    "    Top-k names of books that matchs the query.\n",
    "\n",
    "   \"\"\"   \n",
    "    tokens = tokenize_text(query)\n",
    "    query_weights = {}\n",
    "    for doc_id, token in tf_idf:\n",
    "        if token in tokens:\n",
    "            query_weights[doc_id] = query_weights.get(doc_id, 0) + tf_idf_index[doc_id, token]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    results = []\n",
    "    for i in query_weights[:k]:\n",
    "        results.append(int(file_names[i[0]]))\n",
    "    results.sort()\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73e03da126b73bfff3642ec5261d56fa25c444ea595de51041687efaa60dda41"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
