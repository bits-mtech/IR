{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-performer",
   "metadata": {},
   "source": [
    "## IR Assignment Group-082\n",
    "\n",
    "## Group members\n",
    "\n",
    "<style>\n",
    "table {\n",
    "  font-family: arial, sans-serif;\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "  border: 1px solid #dddddd;\n",
    "  text-align: left;\n",
    "  padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even) {\n",
    "  background-color: #dddddd;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Full Name</th>\n",
    "    <th>BITS ID</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>T V Sriharini</td>\n",
    "    <td>2020fc04020@wilp.bits-pilani.ac.in</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Subodh Kant Mishra</td>\n",
    "    <td>2020fc04064@wilp.bits-pilani.ac.in</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mriganka Shekhar Gayen</td>\n",
    "    <td>2020fc04069@wilp.bits-pilani.ac.in</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Given a Dataset collected from the Centre for Inventions and Scientific Information (\"CISI\") and consists of text data about 1,460 documents and 112 associated queries. Its purpose is to be used to build models of information retrieval where a given query will return a list of document IDs relevant to the query. The file \"CISI.REL\" contains the correct list (ie. \"gold standard\" or \"ground proof\") of query-document matching and your model can be compared against this \"gold standard\" to see how it has performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-awareness",
   "metadata": {},
   "source": [
    "### 1. Load the data set and import necessary libraries (1 Marks)\n",
    "We import various packages used for text and general data processing\n",
    "* Pandas, Numpy, Skleaarn - For Data Processing\n",
    "* NLTK - Natural Language Tool Kit for Natural Language Processing\n",
    "* OS, functools - General Packages for Processing files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "balanced-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kants\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kants\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Library imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#we will use Scikit-Learn's TfidfVectorizer, which operates similarly to CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#implementation using scikit-learn\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "import os, glob, re, sys, random, unicodedata, collections\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-shield",
   "metadata": {},
   "source": [
    "### 1.1 Importing the files - All FIle\n",
    "\n",
    "We are importing the CISI.ALL file where we perform text processing such as removing newline characters, and '.' characters,\n",
    "and split the data into different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b7e294cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path,filetype=\"ALL\"):\n",
    "    \"\"\"\n",
    "    Extracts text from a given file path.\n",
    "    \"\"\"\n",
    "\n",
    "    qry_id=0\n",
    "    doc_id=0\n",
    "    doc_text = \"\"\n",
    "    rel_set = {}\n",
    "    doc_set = {}\n",
    "    qry_set = {}\n",
    "    lines = \"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        if filetype == \"ALL\" or filetype == \"QRY\":\n",
    "            for l in f.readlines():\n",
    "                lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "            lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "        elif filetype == \"REL\":\n",
    "            for l in f.readlines():\n",
    "                qry_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]) -1\n",
    "                doc_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])-1\n",
    "                if qry_id in rel_set:\n",
    "                    rel_set[qry_id].append(doc_id)\n",
    "                else:\n",
    "                    rel_set[qry_id] = []\n",
    "                    rel_set[qry_id].append(doc_id)\n",
    "            return rel_set\n",
    "        else:\n",
    "            print(\"Invalid file type\")\n",
    "            return lines\n",
    "    if filetype == \"ALL\": \n",
    "        for l in lines:\n",
    "            if l.startswith(\".I\"):\n",
    "                doc_id = int(l.split(\" \")[1].strip())-1\n",
    "            elif l.startswith(\".X\"):\n",
    "                doc_set[doc_id] = doc_text.lstrip(\" \")\n",
    "                doc_id = \"\"\n",
    "                doc_text = \"\"\n",
    "            else:\n",
    "                doc_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.\n",
    "        return doc_set\n",
    "    elif filetype == \"QRY\":\n",
    "        qry_id = \"\"\n",
    "        for l in lines:\n",
    "            if l.startswith(\".I\"):\n",
    "                qry_id = int(l.split(\" \")[1].strip()) -1\n",
    "            elif l.startswith(\".W\"):\n",
    "                qry_set[qry_id] = l.strip()[3:]\n",
    "                qry_id = \"\"\n",
    "        return qry_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "weighted-marina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1460 documents\n"
     ]
    }
   ],
   "source": [
    "### Processing DOCUMENTS\n",
    "BASE_FOLDER = \"D://Mtech//Sem3//IR//Assignment//archive//\"\n",
    "doc_set = extract_text(f'{BASE_FOLDER}CISI.ALL',filetype=\"ALL\")\n",
    "print(f'Read {len(doc_set)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-mistake",
   "metadata": {},
   "source": [
    "### 1.2 Importing the files - Queries FIle\n",
    "\n",
    "We are importing the CISI.QRY file where we perform text processing such as removing newline characters, and '.' characters,\n",
    "and split the data into different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "labeled-reaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 112 documents\n"
     ]
    }
   ],
   "source": [
    "qry_set = extract_text(f'{BASE_FOLDER}CISI.QRY',filetype=\"QRY\")\n",
    "print(f'Read {len(qry_set)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-comparative",
   "metadata": {},
   "source": [
    "### 1.3 Importing the files - Rel FIle\n",
    "\n",
    "We are importing the CISI.QREL file where we perform text processing such as removing newline characters, and '.' characters,\n",
    "and split the data into different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "available-fraud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 76 documents\n"
     ]
    }
   ],
   "source": [
    "rel_set = extract_text(f'{BASE_FOLDER}CISI.REL',filetype=\"REL\")\n",
    "print(f'Read {len(rel_set)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-accreditation",
   "metadata": {},
   "source": [
    "### 2.1 Extract features from text files. Each unique word in the document can be considered as a feature (1 Mark)\n",
    "\n",
    "We check the General statistics of the file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "public-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1460 documents, 112 queries and 76 mappings from CISI dataset\n",
      "Average 40.97 and 1 min number of relevant documents by query \n",
      "Queries without relevant documents:  [ 35  37  39  46  47  50  52  58  59  62  63  67  69  71  72  73  74  76\n",
      "  77  79  82  84  85  86  87  88  90  92  93 102 104 105 106 107 109 111]\n"
     ]
    }
   ],
   "source": [
    "## Here we check some statistics and info of CISI dataset\n",
    "\n",
    "print(f'Read {len(doc_set)} documents, {len(qry_set)} queries and {len(rel_set)} mappings from CISI dataset')\n",
    "\n",
    "number_of_rel_docs = [len(value) for key, value in rel_set.items()]\n",
    "print('Average %.2f and %d min number of relevant documents by query ' % \n",
    "      (np.mean(number_of_rel_docs), np.min(number_of_rel_docs)))\n",
    "\n",
    "print('Queries without relevant documents: ', \n",
    "      np.setdiff1d(list(qry_set.keys()),list(rel_set.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667018c",
   "metadata": {},
   "source": [
    "#### Checking the sample of relevant document from query and its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "proper-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID 14 ==> How much do information retrieval and dissemination systems, as well as automated libraries, cost? Are they worth it to the researcher and to industry?\n",
      "Documents relevants to Query ID 14 [17, 26, 35, 48, 55, 58, 66, 73, 82, 125, 157, 163, 166, 191, 213, 221, 222, 249, 280, 291, 294, 298, 306, 330, 335, 337, 347, 364, 365, 366, 367, 371, 380, 445, 457, 464, 465, 481, 489, 490, 494, 496, 506, 519, 527, 590, 593, 622, 628, 638, 689, 719, 722, 723, 726, 727, 730, 778, 821, 833, 838, 847, 848, 864, 871, 896, 1099, 1160, 1247, 1304, 1352, 1357, 1362, 1365, 1367, 1370, 1371, 1373, 1374, 1375, 1376, 1409]\n",
      "Document ID 48 ==> Adaptive Information Dissemination Sage, C.R. Anderson, R.R. Fitzwater, D.R. Computer dissemination of information offers significant advantages over manual dissemination because the computer can use strategies that are impractical and in some cases impossible for a human.. This paper describes the Ames Laboratory Selective Dissemination of Information system with emphasis on the effectiveness of user feedback.. The system will accept any document, abstract, keyword, etc., in a KWIC or Science Citation Index Source format.. User profiles consist of words or word clusters each with an initially assigned significance value.. These values are used in making the decision to notify a user that he may be interested in a particular document.. According to responses, the significance values are increased or decreased and quickly attain an equilibrium which accurately describes the user's interests.. The system is economical compared to other existing SDI systems and human intervention is negligible except for adding and deleting profile entries.. \n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "idx = random.sample(rel_set.keys(),1)[0]\n",
    "\n",
    "print('Query ID %s ==>' % idx, qry_set[idx])\n",
    "rel_docs = rel_set[idx]\n",
    "print('Documents relevants to Query ID %s' % idx, rel_docs)\n",
    "sample_document_idx = random.sample(rel_docs,1)[0]\n",
    "print('Document ID %s ==>' % sample_document_idx, doc_set[sample_document_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8aae61",
   "metadata": {},
   "source": [
    "## Deriving Feature using Count Vectorizer Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "68cc125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Zeros doc matrix 84538\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>029</th>\n",
       "      <th>044</th>\n",
       "      <th>054</th>\n",
       "      <th>071</th>\n",
       "      <th>071182</th>\n",
       "      <th>073</th>\n",
       "      <th>077</th>\n",
       "      <th>084</th>\n",
       "      <th>...</th>\n",
       "      <th>zipf</th>\n",
       "      <th>zipfian</th>\n",
       "      <th>zipperer</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoology</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zunde</th>\n",
       "      <th>zvezhinskii</th>\n",
       "      <th>zyabrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 11030 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  029  044  054  071  071182  073  077  084  ...  zipf  zipfian  \\\n",
       "0      0    0    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "1      0    0    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "2      0    0    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "3      0    0    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "4      0    0    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "...   ..  ...  ...  ...  ...  ...     ...  ...  ...  ...  ...   ...      ...   \n",
       "1455   0    0    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "1456   0    0    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "1457   0    0    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "1458   0    0    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "1459   0    1    0    0    0    0       0    0    0    0  ...     0        0   \n",
       "\n",
       "      zipperer  zone  zones  zoology  zuckerman  zunde  zvezhinskii  zyabrev  \n",
       "0            0     0      0        0          0      0            0        0  \n",
       "1            0     0      0        0          0      0            0        0  \n",
       "2            0     0      0        0          0      0            0        0  \n",
       "3            0     0      0        0          0      0            0        0  \n",
       "4            0     0      0        0          0      0            0        0  \n",
       "...        ...   ...    ...      ...        ...    ...          ...      ...  \n",
       "1455         0     0      0        0          0      0            0        0  \n",
       "1456         0     0      0        0          0      0            0        0  \n",
       "1457         0     0      0        0          0      0            0        0  \n",
       "1458         0     0      0        0          0      0            0        0  \n",
       "1459         0     0      0        0          0      0            0        0  \n",
       "\n",
       "[1460 rows x 11030 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count term frequency using CountVectorizer from scikit-learn\n",
    "## limiting number of words just for illustrating the concept\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "vec = CountVectorizer(stop_words=STOP_WORDS) \n",
    "X = vec.fit_transform(doc_set.values())\n",
    "print('Non Zeros doc matrix',vec.transform(doc_set.values()).count_nonzero())\n",
    "df = pd.DataFrame(X.todense(), columns=vec.get_feature_names())\n",
    "df\n",
    "#word_tokenize(str(preprocess(doc_set[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "217ba659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n"
     ]
    }
   ],
   "source": [
    "processed_text = []\n",
    "processed_set = {}\n",
    "for key in doc_set:\n",
    "    processed_text.append(word_tokenize(str(preprocess(doc_set[key]))))\n",
    "    processed_set[key] = word_tokenize(str(preprocess(doc_set[key])))\n",
    "print(len(processed_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1ba9406c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "60df542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>18</th>\n",
       "      <th>editions</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>dewey</th>\n",
       "      <th>decimal</th>\n",
       "      <th>classifications</th>\n",
       "      <th>comaromi</th>\n",
       "      <th>j</th>\n",
       "      <th>p</th>\n",
       "      <th>...</th>\n",
       "      <th>clause</th>\n",
       "      <th>subsidization</th>\n",
       "      <th>morn</th>\n",
       "      <th>religiously</th>\n",
       "      <th>prompts</th>\n",
       "      <th>soundness</th>\n",
       "      <th>supposition</th>\n",
       "      <th>poluskin</th>\n",
       "      <th>abstraction</th>\n",
       "      <th>certificates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.088967</td>\n",
       "      <td>10.680563</td>\n",
       "      <td>0.086838</td>\n",
       "      <td>0.144880</td>\n",
       "      <td>14.163727</td>\n",
       "      <td>9.027206</td>\n",
       "      <td>4.241669</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>1.809728</td>\n",
       "      <td>2.797555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136459</td>\n",
       "      <td>0.101416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099243</td>\n",
       "      <td>0.086928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.797555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099243</td>\n",
       "      <td>0.130392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099243</td>\n",
       "      <td>0.130392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.797555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>0.202832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.809728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136459</td>\n",
       "      <td>0.260784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099243</td>\n",
       "      <td>0.173856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074432</td>\n",
       "      <td>0.086928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>7.286192</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049622</td>\n",
       "      <td>0.072440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.286192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 11186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            18   editions        of       the      dewey   decimal  \\\n",
       "0     5.088967  10.680563  0.086838  0.144880  14.163727  9.027206   \n",
       "1     0.000000   0.000000  0.136459  0.101416   0.000000  0.000000   \n",
       "2     0.000000   0.000000  0.099243  0.086928   0.000000  0.000000   \n",
       "3     0.000000   0.000000  0.099243  0.130392   0.000000  0.000000   \n",
       "4     0.000000   0.000000  0.099243  0.130392   0.000000  0.000000   \n",
       "...        ...        ...       ...       ...        ...       ...   \n",
       "1455  0.000000   0.000000  0.062027  0.202832   0.000000  0.000000   \n",
       "1456  0.000000   0.000000  0.136459  0.260784   0.000000  0.000000   \n",
       "1457  0.000000   0.000000  0.099243  0.173856   0.000000  0.000000   \n",
       "1458  0.000000   0.000000  0.074432  0.086928   0.000000  0.000000   \n",
       "1459  0.000000   0.000000  0.049622  0.072440   0.000000  0.000000   \n",
       "\n",
       "      classifications  comaromi         j         p  ...    clause  \\\n",
       "0            4.241669  7.286192  1.809728  2.797555  ...  0.000000   \n",
       "1            0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "2            0.000000  0.000000  0.000000  2.797555  ...  0.000000   \n",
       "3            0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "4            0.000000  0.000000  0.000000  2.797555  ...  0.000000   \n",
       "...               ...       ...       ...       ...  ...       ...   \n",
       "1455         0.000000  0.000000  1.809728  0.000000  ...  0.000000   \n",
       "1456         0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "1457         0.000000  0.000000  0.000000  0.000000  ...  7.286192   \n",
       "1458         0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "1459         0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "\n",
       "      subsidization      morn  religiously   prompts  soundness  supposition  \\\n",
       "0          0.000000  0.000000     0.000000  0.000000   0.000000     0.000000   \n",
       "1          0.000000  0.000000     0.000000  0.000000   0.000000     0.000000   \n",
       "2          0.000000  0.000000     0.000000  0.000000   0.000000     0.000000   \n",
       "3          0.000000  0.000000     0.000000  0.000000   0.000000     0.000000   \n",
       "4          0.000000  0.000000     0.000000  0.000000   0.000000     0.000000   \n",
       "...             ...       ...          ...       ...        ...          ...   \n",
       "1455       0.000000  0.000000     0.000000  0.000000   0.000000     0.000000   \n",
       "1456       0.000000  0.000000     0.000000  0.000000   0.000000     0.000000   \n",
       "1457       7.286192  7.286192     7.286192  7.286192   7.286192     7.286192   \n",
       "1458       0.000000  0.000000     0.000000  0.000000   0.000000     0.000000   \n",
       "1459       0.000000  0.000000     0.000000  0.000000   0.000000     0.000000   \n",
       "\n",
       "      poluskin  abstraction  certificates  \n",
       "0     0.000000     0.000000      0.000000  \n",
       "1     0.000000     0.000000      0.000000  \n",
       "2     0.000000     0.000000      0.000000  \n",
       "3     0.000000     0.000000      0.000000  \n",
       "4     0.000000     0.000000      0.000000  \n",
       "...        ...          ...           ...  \n",
       "1455  0.000000     0.000000      0.000000  \n",
       "1456  0.000000     0.000000      0.000000  \n",
       "1457  0.000000     0.000000      0.000000  \n",
       "1458  7.286192     7.286192      0.000000  \n",
       "1459  0.000000     0.000000      7.286192  \n",
       "\n",
       "[1460 rows x 11186 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "series = pd.Series(doc_set)\n",
    "bag_of_words = (\n",
    "    series.\n",
    "    str.lower().                  # convert all letters to lowercase\n",
    "    str.replace(\"[^\\w\\s]\", \" \").  # replace non-alphanumeric characters by whitespace\n",
    "    str.split()                   # split on whitespace\n",
    ").apply(Counter)\n",
    "tf = pd.DataFrame(list(bag_of_words)).fillna(0)\n",
    "\n",
    "df = (tf > 0).sum(axis=0)\n",
    "# Get IDFs\n",
    "idf = np.log(len(tf) / df)\n",
    "# Calculate TF-IDFs\n",
    "tf_idf = tf * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5d48fd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1460x11150 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 114611 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will use Scikit-Learn's TfidfVectorizer, which operates similarly to CountVectorizer\n",
    "vec = TfidfVectorizer(norm=None) # Do not normalize.\n",
    "vec.fit(series) # This determines the vocabulary.\n",
    "tf_idf_sparse = vec.transform(series)\n",
    "tf_idf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c5638",
   "metadata": {},
   "source": [
    "### Calculating Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "36d850a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.10351387, 0.07485884, ..., 0.10059578, 0.08325116,\n",
       "       0.09139324])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tf_idf_sparse)\n",
    "cosine_similarity(tf_idf_sparse)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "fe18f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_MIN_LENGTH = 2 ## we'll drop all tokens with less than this size\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data\n",
    "\n",
    "def strip_accents(text):\n",
    "    \"\"\"Strip accents and punctuation from text. \n",
    "    For instance: strip_accents(\"João e Maria, não entrem!\") \n",
    "    will return \"Joao e Maria  nao entrem \"\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "    str: text without accents and punctuation\n",
    "\n",
    "   \"\"\"    \n",
    "    nfkd = unicodedata.normalize('NFKD', text)\n",
    "    newText = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "    return re.sub('[^a-zA-Z0-9 \\\\\\']', ' ', newText)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Make all necessary preprocessing of text: strip accents and punctuation, \n",
    "    remove \\n, tokenize our text, convert to lower case, remove stop words and \n",
    "    words with less than 2 chars.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "    str: cleaned tokenized text\n",
    "\n",
    "   \"\"\"        \n",
    "    text = strip_accents(text)\n",
    "    text = re.sub(re.compile('\\n'),' ',text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in STOP_WORDS and len(word) >= WORD_MIN_LENGTH]\n",
    "    return words\n",
    "    \n",
    "def inverted_index(words):\n",
    "    \"\"\"Create a inverted index of words (tokens or terms) from a list of terms\n",
    "\n",
    "    Parameters:\n",
    "    words (list of str): tokenized document text\n",
    "\n",
    "    Returns:\n",
    "    Inverted index of document (dict)\n",
    "\n",
    "   \"\"\"       \n",
    "    inverted = {}\n",
    "    for index, word in enumerate(words):\n",
    "        locations = inverted.setdefault(word, [])\n",
    "        locations.append(index)\n",
    "    return inverted\n",
    "\n",
    "def inverted_index_add(inverted, doc_id, doc_index):\n",
    "    \"\"\"Insert document id into Inverted Index\n",
    "\n",
    "    Parameters:\n",
    "    inverted (dict): Inverted Index\n",
    "    doc_id (int): Id of document been added\n",
    "    doc_index (dict): Inverted Index of a specific document.\n",
    "\n",
    "    Returns:\n",
    "    Inverted index of document (dict)\n",
    "\n",
    "   \"\"\"        \n",
    "    for word in doc_index.keys():\n",
    "        locations = doc_index[word]\n",
    "        indices = inverted.setdefault(word, {})\n",
    "        indices[doc_id] = locations\n",
    "    return inverted\n",
    "\n",
    "## Using AND as logical operator\n",
    "def boolean_search(inverted, file_names, query):\n",
    "    \"\"\"Run a boolean search with AND operator between terms over \n",
    "    the inverted index.\n",
    "\n",
    "    Parameters:\n",
    "    inverted (dict): Inverted Index\n",
    "    file_names (list): List with names of files (books)\n",
    "    query (txt): Query text\n",
    "\n",
    "    Returns:\n",
    "    Names of books that matchs the query.\n",
    "\n",
    "   \"\"\"      \n",
    "    # preprocess the user query using same function used to build Inverted Index\n",
    "    words = [word for _, word in enumerate(tokenize_text(query)) if word in inverted]\n",
    "    # list with a disctinct document match for each term from query\n",
    "    results = [set(inverted[word].keys()) for word in words]\n",
    "    # AND operator. Replace & for | to modify to OR behavior.\n",
    "    docs = reduce(lambda x, y: x & y, results) if results else []\n",
    "    return ([file_names[doc] for doc in docs])\n",
    "\n",
    "def ranked_search(k, tf_idf_index, file_names, query):\n",
    "    \"\"\"Run ranked query search using tf-idf model.\n",
    "\n",
    "    Parameters:\n",
    "    k (int): number of results to return\n",
    "    tf_idf_index (dict): Data Structure storing Tf-Idf weights to each \n",
    "                        pair of (term,doc_id) \n",
    "    file_names (list): List with names of files (books)\n",
    "    query (txt): Query text\n",
    "\n",
    "    Returns:\n",
    "    Top-k names of books that matchs the query.\n",
    "\n",
    "   \"\"\"   \n",
    "    tokens = tokenize_text(query)\n",
    "    query_weights = {}\n",
    "    for doc_id, token in tf_idf:\n",
    "        if token in tokens:\n",
    "            query_weights[doc_id] = query_weights.get(doc_id, 0) + tf_idf_index[doc_id, token]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    results = []\n",
    "    for i in query_weights[:k]:\n",
    "        results.append(int(file_names[i[0]]))\n",
    "    results.sort()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e3ad0",
   "metadata": {},
   "source": [
    "## TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "bb0348b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:01<00:00, 981.77it/s] \n"
     ]
    }
   ],
   "source": [
    "inverted_doc_indexes = {}\n",
    "files_with_index = []\n",
    "files_with_tokens = {}\n",
    "doc_id=0\n",
    "for fname in tqdm(doc_set):\n",
    "    text = doc_set[fname]\n",
    "\n",
    "    #Clean and Tokenize text of each document\n",
    "    words = tokenize_text(text)\n",
    "    #Store tokens\n",
    "    files_with_tokens[doc_id] = words\n",
    "\n",
    "    doc_index = inverted_index(words)\n",
    "    inverted_index_add(inverted_doc_indexes, doc_id, doc_index)\n",
    "    files_with_index.append(fname)\n",
    "    doc_id = doc_id+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98fb19",
   "metadata": {},
   "source": [
    "#### Searching from keyword using inverted index set. Also using boolean search to derive the document id and its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1bc1d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1395 \n",
      "----\n",
      " Survey of Commercially Available Computer-Readable Bibliographic Data Bases Schneider, John H. Gechman, Marvin Furth, Stephen E. This document contains the results of a survey of 94 US organizations, and 36 organizations in other countries that were thought to prepare machine-readable data bases.. Of those surveyed, 55 organizations (40 in U.S., 15 in other countries) provided completed camera-ready forms describing bibliographic information about published literature.. The following types of data were requested for each data base:  Name, frequency of issue, and time span covered by the data base; Name of organizations and individuals who can provide information on the data base; Subject matter and scope of data on the tape; Source of information in the data base (journal articles, reports, patents, monographs, etc.); Method(s) used for indexing or other types of subject analysis; Special data elements; Tape specifications (density, tracks, labels, etc.); Availability of programs for retrospective searching and selective dissemination of information (SDI); Type and cost of search services offered; and Availability and charges for data bases.. The information provided represents the status of these data bases as of November 1972.. It is anticipated that libraries and other information centers will find this document helpful in selecting data bases for providing SDI; retrospective search services, and other bibliographic reference services to their users..  \n",
      "----\n",
      "[1105]\n"
     ]
    }
   ],
   "source": [
    "## Check presence of zyabrev token into 1105 doocument:\n",
    "zdocs = inverted_doc_indexes['centers']\n",
    "zdocs_id = 0\n",
    "for idx in zdocs.keys():\n",
    "    zdocs_id = files_with_index[idx]\n",
    "print(zdocs_id,'\\n----\\n' ,doc_set[zdocs_id],'\\n----')\n",
    "# Passage from \"1105 doc\"\n",
    "print(boolean_search(inverted_doc_indexes, files_with_index, \"survey, questionnaire; electronics, system\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "298227a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 2823.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84840"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Number of documents each term occurs\n",
    "DF = {}\n",
    "for word in inverted_doc_indexes.keys():\n",
    "    DF[word] = len ([doc for doc in inverted_doc_indexes[word]])\n",
    "\n",
    "total_vocab_size = len(DF)\n",
    "print(total_vocab_size)\n",
    "\n",
    "tf_idf = {} # Our data structure to store Tf-Idf weights\n",
    "N = len(files_with_tokens)\n",
    "for doc_id, tokens in tqdm(files_with_tokens.items()):\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        # Calculate Tf\n",
    "        tf = counter[token] # Counter returns a tuple with each terms counts\n",
    "        tf = 1+np.log(tf)\n",
    "        # Calculate Idf\n",
    "        if token in DF:\n",
    "            df = DF[token]\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        # Calculate Tf-idf        \n",
    "        tf_idf[doc_id, token] = tf*idf\n",
    "len(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0e2c8758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests? \n",
      "--- Searched Doc Ids ---\n",
      " [64, 165, 201, 209, 380, 522, 602, 635, 787, 789, 805, 809, 1053, 1054, 1090, 1154, 1157, 1275, 1398, 1417] ----------\n",
      " [28, 67, 196, 212, 213, 308, 318, 323, 428, 498, 635, 668, 669, 673, 689, 691, 694, 699, 703, 708, 719, 730, 732, 737, 739, 1135]\n"
     ]
    }
   ],
   "source": [
    "print(qry_set[1],'\\n--- Searched Doc Ids ---\\n',ranked_search(20, tf_idf, files_with_index, qry_set[1]),'----------\\n',rel_set[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefaaea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae1a6e4a",
   "metadata": {},
   "source": [
    "### Pseudo Relevance Feedback Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "fourth-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ==>  What is the need for information consolidation, evaluation, and retrieval in scientific research? \n",
      "Relevant documents IDs: ==>  [1, 14, 31, 32, 36, 40, 62, 66, 86, 89, 94, 109, 112, 115, 120, 152, 159, 162, 164, 165, 168, 171, 198, 201, 214, 218, 222, 242, 251, 253, 265, 308, 313, 326, 344, 354, 355, 359, 370, 371, 372, 380, 385, 390, 394, 397, 414, 438, 439, 452, 455, 512, 532, 543, 546, 559, 601, 604, 655, 656, 665, 713, 715, 717, 718, 722, 727, 730, 732, 736, 737, 738, 762, 770, 771, 787, 788, 820, 827, 836, 838, 844, 898, 951, 952, 964, 966, 1079, 1080, 1088, 1093, 1094, 1095, 1097, 1098, 1101, 1109, 1112, 1113, 1120, 1121, 1127, 1150, 1154, 1155, 1157, 1159, 1222, 1227, 1234, 1240, 1253, 1255, 1256, 1263, 1267, 1274, 1283, 1294, 1297, 1302, 1308, 1318, 1372, 1403, 1417, 1445]\n",
      "bm25 scores ----->  [11.35530779 12.61482487 14.49104357 ... 11.2859353   9.49749892\n",
      " 16.78178902]\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "query = qry_set[idx] #get query text\n",
    "rel_docs = rel_set[idx] #get relevant documents\n",
    "\n",
    "# Index all documents using BM25\n",
    "corpus = list(doc_set.values())\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Process query and get scores for each indexed document using BM25\n",
    "tokenized_query = query.split(\" \")\n",
    "print('Query ==> ', query, '\\nRelevant documents IDs: ==> ', rel_docs)\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "print('bm25 scores -----> ',scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "banned-alloy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611111111111111"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Source: https://gist.github.com/bwhite/3726239\n",
    "def mean_reciprocal_rank(bool_results, k=10):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    bool_results = (np.atleast_1d(r[:k]).nonzero()[0] for r in bool_results)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in bool_results])\n",
    "\n",
    "mean_reciprocal_rank([[0, 0, 1], [0, 1, 0], [1, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "dynamic-recipient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 460  134  574  458  387  133  593  595  543  610  397   59  559  894\n",
      "  480   23  363  898  629  444 1143  524 1050  982  500 1357  636  250\n",
      "  165  584  124  382  706  614  455  647  266  334  946 1200  483 1088\n",
      " 1119 1079  975 1222  685  635  326 1406  370   65  452  546  895    5\n",
      " 1447 1174  663  175  937 1163 1348 1161 1179  173  128  495   70  239\n",
      " 1122 1093  947 1459  485  715  705  964  496  537  797  340  167  457\n",
      "  113  754 1157 1308 1123  313  514 1185  174  812    7  322 1166 1125\n",
      "  141  258]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "## Argsort gives the indexes of values in increasing order, so we input with the negative values of scores\n",
    "most_relevant_documents = np.argsort(-scores)\n",
    "\n",
    "print(most_relevant_documents[:100]) # printing first 100 most relevant results\n",
    "\n",
    "## Mask relevant documents with 0's and 1's according to query <-> document annotation\n",
    "masked_relevance_results = np.zeros(most_relevant_documents.shape)\n",
    "masked_relevance_results[rel_docs] = 1\n",
    "sorted_masked_relevance_results = np.take(masked_relevance_results, most_relevant_documents)\n",
    "\n",
    "print(sorted_masked_relevance_results[:100]) #printing first 100 results: 1 is relevant 0 isn't\n",
    "\n",
    "# Calculate MRR@10\n",
    "print(mean_reciprocal_rank([sorted_masked_relevance_results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "brown-salvation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 0.1483\n"
     ]
    }
   ],
   "source": [
    "def results_from_query(qry_id, bm25):\n",
    "    \"\"\"Return an ordered array of relevant documents returned by query_id\n",
    "\n",
    "    Args:\n",
    "        qry_id (int): id of query on dataset\n",
    "        bm25 (object): indexed corpus\n",
    "\n",
    "    Returns:\n",
    "        boolean sorted relevance array of documents\n",
    "    \"\"\"    \n",
    "    query = qry_set[qry_id]\n",
    "    rel_docs = []\n",
    "    if qry_id in rel_set:\n",
    "        rel_docs = rel_set[qry_id]\n",
    "    tokenized_query = query.split(\" \")\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    most_relevant_documents = np.argsort(-scores)\n",
    "    masked_relevance_results = np.zeros(most_relevant_documents.shape)\n",
    "  \n",
    "    masked_relevance_results[rel_docs] = 1\n",
    "    sorted_masked_relevance_results = np.take(masked_relevance_results, most_relevant_documents)\n",
    "    \n",
    "    return sorted_masked_relevance_results\n",
    "\n",
    "\n",
    "results = [results_from_query(qry_id, bm25) for qry_id in list(qry_set.keys())]\n",
    "print('MRR@10 %.4f' % mean_reciprocal_rank(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "rolled-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instaciate objects from NLTK\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "radical-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(txt, remove_stop=True, do_stem=True, to_lower=True):\n",
    "    \"\"\"\n",
    "    Return a preprocessed tokenized text.\n",
    "    \n",
    "    Args:\n",
    "        txt (str): original text to process\n",
    "        remove_stop (boolean): to remove or not stop words (common words)\n",
    "        do_stem (boolean): to do or not stemming (suffixes and prefixes removal)\n",
    "        to_lower (boolean): remove or not capital letters.\n",
    "        \n",
    "    Returns:\n",
    "        Return a preprocessed tokenized text.\n",
    "    \"\"\"    \n",
    "    if to_lower:\n",
    "        txt = txt.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(txt)\n",
    "    \n",
    "    if remove_stop:\n",
    "        tokens = [tk for tk in tokens if tk not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(tk) for tk in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "standard-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@10 0.4187\n"
     ]
    }
   ],
   "source": [
    "corpus = list(doc_set.values())\n",
    "# You may experiment with this trying to improve MRR@10\n",
    "remove_stop = True\n",
    "do_stem = True\n",
    "to_lower = True\n",
    "\n",
    "tokenized_corpus = [preprocess_string(doc, remove_stop, do_stem, to_lower) for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def results_from_query_new(qry_id, bm25):\n",
    "    query = qry_set[qry_id]\n",
    "    rel_docs = []\n",
    "    if qry_id in rel_set:\n",
    "        rel_docs = rel_set[qry_id]\n",
    "    tokenized_query = preprocess_string(query, remove_stop, do_stem, to_lower)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    most_relevant_documents = np.argsort(-scores)\n",
    "    masked_relevance_results = np.zeros(most_relevant_documents.shape)\n",
    "corpus = list(doc_set.values())\n",
    "# You may experiment with this trying to improve MRR@10\n",
    "remove_stop = True\n",
    "do_stem = True\n",
    "to_lower = True\n",
    "\n",
    "tokenized_corpus = [preprocess_string(doc, remove_stop, do_stem, to_lower) for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def results_from_query_new(qry_id, bm25):\n",
    "    query = qry_set[qry_id]\n",
    "    rel_docs = []\n",
    "    if qry_id in rel_set:\n",
    "        rel_docs = rel_set[qry_id]\n",
    "    tokenized_query = preprocess_string(query, remove_stop, do_stem, to_lower)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    most_relevant_documents = np.argsort(-scores)\n",
    "    masked_relevance_results = np.zeros(most_relevant_documents.shape)\n",
    "    masked_relevance_results[rel_docs] = 1\n",
    "    sorted_masked_relevance_results = np.take(masked_relevance_results, most_relevant_documents)\n",
    "    return sorted_masked_relevance_results\n",
    "\n",
    "\n",
    "results = [results_from_query_new(qry_id, bm25) for qry_id in list(qry_set.keys())]\n",
    "print('MRR@10 %.4f' % mean_reciprocal_rank(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-latino",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-workshop",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73e03da126b73bfff3642ec5261d56fa25c444ea595de51041687efaa60dda41"
  },
  "kernelspec": {
   "display_name": "Julia 1.6.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
